{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Armory-Library Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### In this tutorial, we will demonstrate how to run an example in armory-library with an example model and dataset. In this tutorial we will use the mnist dataset for simplicity and a model via HuggingFace that was trained on the mnist dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLVcyZgR41Gi"
      },
      "source": [
        "### The necessary imports to run an armory library example\n",
        "### We install datasets because we will use it later to retrieve a dataset from the HuggingFace hub. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFB-V0Y1RYJ3",
        "outputId": "ee82d90c-bf14-4f46-866a-41f2b80e228b"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'binja' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/Users/jonathan.prokos/.venv/binja/bin/python -m pip install ipykernel -U --force-reinstall'"
          ]
        }
      ],
      "source": [
        "!pip install armory-library\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVyMdIKHzIQq"
      },
      "source": [
        "### Imports for example\n",
        "### These are all the necessary imports to run the example file and will all be used and explained later in the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u5akHz7F98hN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-04 04:10:16.743910: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-01-04 04:10:16.782833: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-04 04:10:16.782889: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-04 04:10:16.783772: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-04 04:10:16.789903: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-04 04:10:17.616927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "from art.attacks.evasion import ProjectedGradientDescent\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "import datasets\n",
        "import torch\n",
        "import torch.nn\n",
        "import torchmetrics.classification\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "\n",
        "from armory.metrics.compute import BasicProfiler\n",
        "from charmory.data import ArmoryDataLoader\n",
        "from charmory.engine import EvaluationEngine\n",
        "import charmory.evaluation as ev\n",
        "from charmory.metrics.perturbation import PerturbationNormMetric\n",
        "from charmory.model.image_classification import JaticImageClassificationModel\n",
        "from charmory.perturbation import ArtEvasionAttack\n",
        "from charmory.tasks.image_classification import ImageClassificationTask\n",
        "from charmory.track import track_init_params, track_params\n",
        "from charmory.utils import Unnormalize\n",
        "import mlflow\n",
        "from PIL import  Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMbJc0qCzOAu"
      },
      "source": [
        "### We need to bring in the model and attach wrappers to run in this example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSrkcW1Gbvvt",
        "outputId": "44226ee0-1905-4bff-a215-42fa12629212"
      },
      "outputs": [],
      "source": [
        "from art.defences.preprocessor import JpegCompression\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class JpegCompressionNormalized(JpegCompression):\n",
        "    \"\"\"\n",
        "    Unnormalize inputs that were normalized during preprocessing,\n",
        "    process use ART JpegCompression, and renormalize\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        clip_values,\n",
        "        quality=50,\n",
        "        apply_fit=True,\n",
        "        apply_predict=False,\n",
        "        means=None,\n",
        "        stds=None,\n",
        "        dtype=np.float32,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            clip_values,\n",
        "            quality=quality,\n",
        "            apply_fit=apply_fit,\n",
        "            apply_predict=apply_predict,\n",
        "        )\n",
        "        if means is None:\n",
        "            means = (0.0, 0.0, 0.0)  # identity operation\n",
        "        if len(means) != 3:\n",
        "            raise ValueError(\"means must have 3 values, one per channel\")\n",
        "        self.means = np.array(means, dtype=dtype)\n",
        "\n",
        "        if stds is None:\n",
        "            stds = (1.0, 1.0, 1.0)  # identity operation\n",
        "        if len(stds) != 3:\n",
        "            raise ValueError(\"stds must have 3 values, one per channel\")\n",
        "        self.stds = np.array(stds, dtype=dtype)\n",
        "\n",
        "    def __call__(self, x, y=None):\n",
        "        x = x * self.stds + self.means\n",
        "        x = torch.clamp(x, self.clip_values[0], self.clip_values[1], out=x)\n",
        "        x, _ = super().__call__(x)\n",
        "        x = (x - self.means) / self.stds\n",
        "        return x, y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = JaticImageClassificationModel(\n",
        "    track_params(AutoModelForImageClassification.from_pretrained)(\n",
        "        \"farleyknight-org-username/vit-base-mnist\"\n",
        "    ),\n",
        "    preadapter=JpegCompressionNormalized(\n",
        "        clip_values=(0, 1),\n",
        "        quality=50,\n",
        "        means=(0.0, 0.0, 0.0),\n",
        "        stds=(1.0, 1.0, 1.0),\n",
        "    ),\n",
        ")\n",
        "classifier = track_init_params(PyTorchClassifier)(\n",
        "    model,\n",
        "    loss=torch.nn.CrossEntropyLoss(),\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.003),\n",
        "    input_shape=(3, 224, 224),\n",
        "    channels_first=True,\n",
        "    nb_classes=10,\n",
        "    clip_values=(-1, 1),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "hIOG76MJAPVA",
        "outputId": "c0d75be3-0e0c-4f0f-c354-ff6c03165800"
      },
      "source": [
        "For this section of code we create two variables `model` and `classifier`. We found a model on HuggingFace that is trained on the same mnist dataset with the model card `farleyknight-org-username/vit-base-mnist`. This can be replaced with another model off of Huggingface or you can use a custom local model.\n",
        "- The model variable uses `AutoModelForImageClassification.from_pretrained` which takes in a HuggingFace model card name as a variable. This retrieves the model from\n",
        "    HuggingFace that we will use for this example. `track_params` is a function wrapper that stores the argument values as parameters in MLflow. Lastly,\n",
        "    the `JaticImageClassificationModel` is another wrapper to make the model compatible with Armory. This allows the model to have a standard output like other\n",
        "    JATIC image classification models.\n",
        "- The `PyTorchClassifier` class wraps the model to be usable by the ART library. It is specific to image classifier models written within the PyTorch framework. It takes in as arguments the model, loss function, and optimizer. The input image sizes are the shape of all the images inside the dataset. The `channels_first` variable is true because the images in the MNIST dataset are in a channels-first (C, H, W) multi-dimensional array. The `nb_classes` describe the number of classes model predicts on. Lastly the clip value  is the values that will be the min and max values of the input after scaling.We use `track_init_params` so that the constructor parameters for the ART wrapper are also tracked in MLflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcJnRKIcZKCm"
      },
      "source": [
        "### We need to get the dataset to use in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qc0-c_vsHZaW",
        "outputId": "94355612-18d6-4687-cf29-76bd61506c86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
          ]
        }
      ],
      "source": [
        "    dataset = datasets.load_dataset(\"mnist\", split=\"test\")\n",
        "    processor = AutoImageProcessor.from_pretrained(\n",
        "        \"farleyknight-org-username/vit-base-mnist\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "l1kLRy30ZB9-",
        "outputId": "87364f6e-1b62-4f75-cb1f-df361928a6bd"
      },
      "source": [
        "\n",
        "For this section of code we create two variables `dataset` and `processor`.\n",
        "- The `dataset` variable uses the datasets module directly from the HuggingFace API. The path location \"mnist\" is the location that of the same `dataset` that was used\n",
        "  to train the model in this example. The parameter `split='test'` means that only the testing dataset is loading into the variable dataset. This is used because\n",
        "  armory-library only using the testing dataset to test the adversarial robustness of the model.\n",
        "- The `processor` variable obtains the model processor used by the model to use in our example. The is needed because the dataset needs to be in the correct form to be\n",
        "  inputted into the model. The path passed into this function is a HuggingFace model card location that is the same location of the model in this example.  This function\n",
        "  just obtains the `preprocessor`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMmIpem9bX_s"
      },
      "source": [
        "### The dataset needs to be preprocessed in the correct form for both armory-library and to be inputted into the model. A pytorch dataloader is needed to use the ART API correcly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53pWumP9JBvX",
        "outputId": "f3eba87f-6706-456e-84f8-8efe550cbddb"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "def transform(sample):\n",
        "        # Use the HF image processor and convert from BW To RGB\n",
        "        sample[\"image\"] = processor([img.convert(\"RGB\") for img in sample[\"image\"]])[\n",
        "            \"pixel_values\"\n",
        "        ]\n",
        "        return sample\n",
        "\n",
        "dataset.set_transform(transform)\n",
        "dataloader = ArmoryDataLoader(dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-2z5YFZBbj3W",
        "outputId": "83517576-4cac-4444-a338-0904f6301064"
      },
      "source": [
        "\n",
        "For this section of code we transform the `dataset` and create a `dataloader`.\n",
        "- To transform the `dataset` we define a function `transform` to cycle through the `dataset` and convert each image into HuggingFace 'RGB' form. The transform function is\n",
        "  using the HuggingFace API where you first create a function that cycles through a `dataset` and preforms the operation on the `dataset` that is needed to get it in correct\n",
        "  form. Next the `set_transform` method is used by the HuggingFace dataset which applies the transform function to the entire HuggingFace dataset.\n",
        "- The `dataloader` variable is created by using the `ArmoryDataLoader` function. The `ArmoryDataloader` is a custom PyTorch DataLoader that produces numpy arrays instead of\n",
        "  Tensors which is required by ART. The only inputted used in this example is the `dataset` and batch size of images. All other variable can be passed in from the PyTorch\n",
        "  DataLoader API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lu7uKqMcGut"
      },
      "source": [
        "## Next an attack is needed for armory-library that we will use to test the adversarial robustness of the machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_n5ASOQah3d",
        "outputId": "11a139d9-48b1-4fd1-ebdf-f5b90e57dac9"
      },
      "outputs": [],
      "source": [
        "    attack = track_init_params(ProjectedGradientDescent)(\n",
        "        classifier,\n",
        "        batch_size=batch_size,\n",
        "        eps=0.031,\n",
        "        eps_step=0.007,\n",
        "        max_iter=20,\n",
        "        num_random_init=1,\n",
        "        random_eps=False,\n",
        "        targeted=False,\n",
        "        verbose=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "OPk221EicVNZ",
        "outputId": "f2ef812e-ebf6-4227-9be7-a3068e7430b7"
      },
      "source": [
        "\n",
        "For this section of code we create an `attack` variable.\n",
        "- The `attack` variable is create with the `ProjectedGradientDescent` class which comes from the ART library. Here is a link to the paper it was modeled after\n",
        "  https://arxiv.org/abs/1706.06083. The track_init_params is used to output the initial metrics to mlflow. It takes as an input the default specs for this `attack`, but\n",
        "  these can be changed to be optimized to other models and datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS5BjHw3dj6c"
      },
      "source": [
        "## A method is needed now to bring together the model, dataset, and attack that will execute all the code to test the adversarial robussness of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R0kMTvQ6vLIW"
      },
      "outputs": [],
      "source": [
        "evaluation = ev.Evaluation(\n",
        "    name=\"mnist-vit-pgd\",\n",
        "    description=\"MNIST image classification using a ViT model and PGD attack\",\n",
        "    author=\"TwoSix\",\n",
        "    dataset=ev.Dataset(\n",
        "        name=\"MNIST\",\n",
        "        x_key=\"image\",\n",
        "        y_key=\"label\",\n",
        "        test_dataloader=dataloader,\n",
        "    ),\n",
        "    model=ev.Model(\n",
        "        name=\"ViT\",\n",
        "        model=classifier,\n",
        "    ),\n",
        "    perturbations={\n",
        "        \"benign\": [],\n",
        "        \"attack\": [\n",
        "            ArtEvasionAttack(\n",
        "                name=\"PGD\",\n",
        "                attack=attack,\n",
        "                use_label_for_untargeted=False,\n",
        "            ),\n",
        "        ],\n",
        "    },\n",
        "    metric=ev.Metric(\n",
        "        profiler=BasicProfiler(),\n",
        "        perturbation={\n",
        "            \"linf_norm\": PerturbationNormMetric(ord=torch.inf),\n",
        "        },\n",
        "        prediction={\n",
        "            \"accuracy\": torchmetrics.classification.Accuracy(\n",
        "                task=\"multiclass\", num_classes=10\n",
        "            ),\n",
        "        },\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9y-A_Imhd2Js",
        "outputId": "9bcc6d30-f659-4344-935e-ea6f31b0f899"
      },
      "source": [
        "\n",
        "For this section of code we create an `evalution` variable.\n",
        "- The `evalution` variable is created by `ev.Evalution` class. This variable holds all the essential elements to run an adversarial robustness attack. The elements are:\n",
        "```python\n",
        "    name: 'user created name of the evalution (str)'\n",
        "    description: 'description of the evalution (str)'\n",
        "    model: ev.Model \n",
        "    dataset: ev.Dataset\n",
        "    author: 'string of the author of the evalution (str)'\n",
        "    perturbations: 'dictionary of names to lists of perturbations'\n",
        "    metric:  ev.Metric\n",
        "```\n",
        "\n",
        "- The `model`, `dataset`, and `attack` were created in the earlier cells. The `metric` variable is using the basic standard metric function for image classification models in\n",
        "  armory-library. In the `dataset` variable, the `x_key` and `y_key` need to be properly listed for the `dataset` being used. The `x_key` needs to be the column name of the image\n",
        "  in the `dataset`. The `y_key` needs to be the column name of the label in the `dataset`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkpJQ_S1eeBc"
      },
      "source": [
        "### Some final detials are needed to output metrics to mlflow and the an engine class is needed to run the evalution class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR9us5nlv_qx",
        "outputId": "af3a2db7-4626-41ab-9225-bcf0fa410733"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
            "INFO: GPU available: True (cuda), used: True\n",
            "INFO: TPU available: False, using: 0 TPU cores\n",
            "INFO: IPU available: False, using: 0 IPUs\n",
            "INFO: HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "export_every_n_batches = 10\n",
        "num_batches = 10\n",
        "task = ImageClassificationTask(\n",
        "        evaluation,\n",
        "        export_adapter=Unnormalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "        export_every_n_batches=export_every_n_batches,\n",
        "    )\n",
        "engine = EvaluationEngine(task, limit_test_batches=num_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d1isa0GTeb3q",
        "outputId": "1ea8a3a4-6302-4499-af4c-2ba8ef518957"
      },
      "source": [
        "\n",
        "For this section of code we create a `task` and `engine`:\n",
        "- The task tells ART the type of model the attack is running on. Armory-library currently has two tasks image classification and object detection. It takes the `evalution`\n",
        "  variable, number of classes in the model, export adapter, and `export_every_n_batches`. The `export_every_n_batches` variable is the amount of batches that get exported to mlFlow, so if\n",
        "  10 is selected then every 10 batches are outputted on mlFlow. The adapter takes in the Unnormalize variable which preforms the inverse of the PyTorch Normalize function. This\n",
        "  operation allows the normalize dataset to be reversed to allow the unnormalized data to be displayed in mlflow.\n",
        "- The `engine` variable is created from the `Evaluation` Engine which takes the task and `limit_test_batches` as variables. The `limit_test_batches` is the amount of batches of testing\n",
        "  data that the engine will create metrics for. The `Evaluation` Engine only has one method `.run()` which runs the experiment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR0yxmsTd9IJ"
      },
      "source": [
        "#### Lastly, the engine variable executes .run() which runs the experiment. Function pprint is used to print the output of the run in a better format than regular python print."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288,
          "referenced_widgets": [
            "471b0b5d882b4f8c95424d5f4c99ba5e",
            "a0e368d043e54fb9945740d3337a4914",
            "081528a43b4f4164a1582a667b4cf596",
            "0dc47e26d6824780a4c312a86caeb1b9",
            "8f742813a4894ed88bf706b96fd42b5f",
            "0eddae49b22c4502851223e534526c66",
            "1744f1d198054a48b232b8df20472baf",
            "799fea83a3ef442caa87610e680f4cbc",
            "c4788d5937494ffc868771c34a09dc10",
            "c91993df4bb14b5c949fdecd2aadd85c",
            "12ea3af3241942a3a16b5eb5142463ff"
          ]
        },
        "id": "isu6NOtlwCFN",
        "outputId": "51f704a2-8266-4c38-b5a8-6240b27b3186"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/01/04 04:10:22 WARNING mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics because creating `GPUMonitor` failed with error: `pynvml` is not installed, to log GPU metrics please run `pip install pynvml` to install it..\n",
            "2024/01/04 04:10:22 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "/home/jonathan.prokos/.venv/armory-library/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=95` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5472f8427a14368a0a3870b2b9a091e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">      attack/accuracy      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.02500000037252903    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     attack/linf_norm      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.03100001811981201    </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">      benign/accuracy      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     benign/linf_norm      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m     attack/accuracy     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.02500000037252903   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m    attack/linf_norm     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.03100001811981201   \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m     benign/accuracy     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m    benign/linf_norm     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024/01/04 04:11:03 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
            "2024/01/04 04:11:03 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'compute': {'Avg. CPU time (s) for 10 executions of attack/perturbation': 3.7299391730921343,\n",
            "             'Avg. CPU time (s) for 10 executions of attack/perturbation/PGD': 3.729918852308765,\n",
            "             'Avg. CPU time (s) for 10 executions of attack/predict': 0.10566661852644757,\n",
            "             'Avg. CPU time (s) for 10 executions of benign/perturbation': 1.118588261306286e-06,\n",
            "             'Avg. CPU time (s) for 10 executions of benign/predict': 0.1220854700775817},\n",
            " 'metrics': {'attack/accuracy': tensor(0.0250),\n",
            "             'attack/linf_norm': tensor(0.0310),\n",
            "             'benign/accuracy': tensor(1.),\n",
            "             'benign/linf_norm': tensor(0.)}}\n"
          ]
        }
      ],
      "source": [
        "pprint(engine.run())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Simple script to retrieve 1 example of a benign and attacked image from the mlflow artifacts\n",
        "mlflow_var = mlflow.search_experiments(filter_string=\"name = 'mnist-vit-pgd'\")[0]\n",
        "path = mlflow_var.artifact_location[7:] + '/' + engine.run_id + '/artifacts'\n",
        "image_bin, image_adv = None, None\n",
        "for f in os.listdir(path):\n",
        "    if f.endswith(\"ex_0_x_adv.png\"):\n",
        "        image_adv = Image.open(os.path.join(path,f))\n",
        "    elif f.endswith(\"ex_0_x.png\"):\n",
        "        image_bin = Image.open(os.path.join(path,f))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benign Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attacked Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_adv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "081528a43b4f4164a1582a667b4cf596": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_799fea83a3ef442caa87610e680f4cbc",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4788d5937494ffc868771c34a09dc10",
            "value": 10
          }
        },
        "0dc47e26d6824780a4c312a86caeb1b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c91993df4bb14b5c949fdecd2aadd85c",
            "placeholder": "​",
            "style": "IPY_MODEL_12ea3af3241942a3a16b5eb5142463ff",
            "value": " 10/10 [01:56&lt;00:00,  0.09it/s]"
          }
        },
        "0eddae49b22c4502851223e534526c66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12ea3af3241942a3a16b5eb5142463ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1744f1d198054a48b232b8df20472baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "471b0b5d882b4f8c95424d5f4c99ba5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0e368d043e54fb9945740d3337a4914",
              "IPY_MODEL_081528a43b4f4164a1582a667b4cf596",
              "IPY_MODEL_0dc47e26d6824780a4c312a86caeb1b9"
            ],
            "layout": "IPY_MODEL_8f742813a4894ed88bf706b96fd42b5f"
          }
        },
        "799fea83a3ef442caa87610e680f4cbc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f742813a4894ed88bf706b96fd42b5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "a0e368d043e54fb9945740d3337a4914": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0eddae49b22c4502851223e534526c66",
            "placeholder": "​",
            "style": "IPY_MODEL_1744f1d198054a48b232b8df20472baf",
            "value": "Testing DataLoader 0: 100%"
          }
        },
        "c4788d5937494ffc868771c34a09dc10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c91993df4bb14b5c949fdecd2aadd85c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
