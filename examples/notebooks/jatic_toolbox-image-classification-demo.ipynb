{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the model from HuggingFace using the JATIC-toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 15:57:06.505928: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-26 15:57:07.836539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/kyle-treubig/Code/jatic/armory/examples/.venv/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from jatic_toolbox import load_model\n",
    "\n",
    "model = load_model(\n",
    "    provider=\"huggingface\",\n",
    "    model_name=\"Kaludi/food-category-classification-v2.0\",\n",
    "    task=\"image-classification\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model returns a `HuggingFaceProbs` object type, but ART expects the model output to be the `y` tensor. So we have to adapt the model to produce the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charmory.utils import adapt_jatic_image_classification_model_for_art\n",
    "\n",
    "adapt_jatic_image_classification_model_for_art(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then move the model to the target runtime defice and wrap it in an ART classifier to make it compatible with Armory/ART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "classifier = PyTorchClassifier(\n",
    "    model,\n",
    "    loss=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.003),\n",
    "    input_shape=(224, 224, 3),\n",
    "    channels_first=False,\n",
    "    nb_classes=12,\n",
    "    clip_values=(0.0, 1.0),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the dataset from from HuggingFace using the JATIC-toolbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imagefolder (/home/kyle-treubig/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
      "Loading cached processed dataset at /home/kyle-treubig/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-b82aa949443f821a.arrow\n"
     ]
    }
   ],
   "source": [
    "from jatic_toolbox import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    provider=\"huggingface\",\n",
    "    dataset_name=\"Kaludi/food-category-classification-v2.0\",\n",
    "    task=\"image-classification\",\n",
    "    split=\"validation\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this dataset contains bad images that will result in errors during evaluation, we will apply a filter to the underlying HuggingFace dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/kyle-treubig/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-20e62b43d7f7d11a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length prior to filtering: 300\n",
      "Dataset length after filtering: 280\n"
     ]
    }
   ],
   "source": [
    "from transformers.image_utils import infer_channel_dimension_format\n",
    "import numpy as np\n",
    "\n",
    "def filter(sample):\n",
    "    try:\n",
    "        infer_channel_dimension_format(np.asarray(sample[\"image\"]))\n",
    "        return True\n",
    "    except Exception as err:\n",
    "        print(err)\n",
    "        return False\n",
    "\n",
    "print(f\"Dataset length prior to filtering: {len(dataset)}\")\n",
    "dataset._dataset = dataset._dataset.filter(filter)\n",
    "print(f\"Dataset length after filtering: {len(dataset)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then prepare a transform for the data using the preprocessor that comes with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charmory.utils import create_jatic_image_classification_dataset_transform\n",
    "\n",
    "transform = create_jatic_image_classification_dataset_transform(model.preprocessor)\n",
    "dataset.set_transform(transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create an Armory data generator around the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-26 15:57:29  0s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1706\u001b[0m `tfds.core.add_checksums_dir` is deprecated. Refactor dataset in self-contained folders (`my_dataset/` folder containing my_dataset.py, my_dataset_test.py, dummy_data/, checksums.tsv). The checksum file will be automatically detected. More info at: https://www.tensorflow.org/datasets/add_dataset\n"
     ]
    }
   ],
   "source": [
    "from charmory.data import JaticVisionDatasetGenerator\n",
    "\n",
    "generator = JaticVisionDatasetGenerator(\n",
    "    dataset=dataset,\n",
    "    batch_size=16,\n",
    "    epochs=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we will define the Armory evaluation, including the attack and scenario to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art.attacks.evasion\n",
    "from charmory.evaluation import (\n",
    "    Attack,\n",
    "    Dataset,\n",
    "    Evaluation,\n",
    "    Metric,\n",
    "    Model,\n",
    "    Scenario,\n",
    "    SysConfig,\n",
    ")\n",
    "import charmory.scenarios.image_classification\n",
    "\n",
    "eval_dataset = Dataset(\n",
    "    name=\"food-category-classification\",\n",
    "    test_dataset=generator,\n",
    ")\n",
    "\n",
    "eval_model = Model(\n",
    "    name=\"food-category-classification\",\n",
    "    model=classifier,\n",
    ")\n",
    "\n",
    "eval_attack = Attack(\n",
    "    function=art.attacks.evasion.ProjectedGradientDescent,\n",
    "    kwargs={\n",
    "        \"batch_size\": 1,\n",
    "        \"eps\": 0.031,\n",
    "        \"eps_step\": 0.007,\n",
    "        \"max_iter\": 20,\n",
    "        \"num_random_init\": 1,\n",
    "        \"random_eps\": False,\n",
    "        \"targeted\": False,\n",
    "        \"verbose\": False,\n",
    "    },\n",
    "    knowledge=\"white\",\n",
    "    use_label=True,\n",
    "    type=None,\n",
    ")\n",
    "\n",
    "eval_scenario = Scenario(\n",
    "    function=charmory.scenarios.image_classification.ImageClassificationTask,\n",
    "    kwargs={},\n",
    ")\n",
    "\n",
    "eval_metric = Metric(\n",
    "    profiler_type=\"basic\",\n",
    "    supported_metrics=[\"accuracy\"],\n",
    "    perturbation=[\"linf\"],\n",
    "    task=[\"categorical_accuracy\"],\n",
    "    means=True,\n",
    "    record_metric_per_sample=False,\n",
    ")\n",
    "\n",
    "eval_sysconfig = SysConfig(\n",
    "    gpus=[\"all\"],\n",
    "    use_gpu=True,\n",
    ")\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    name=\"food-category-classification\",\n",
    "    description=\"Food category classification from HuggingFace\",\n",
    "    author=\"Kaludi\",\n",
    "    dataset=eval_dataset,\n",
    "    model=eval_model,\n",
    "    attack=eval_attack,\n",
    "    scenario=eval_scenario,\n",
    "    metric=eval_metric,\n",
    "    sysconfig=eval_sysconfig,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create an engine for the evaluation and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 18/18 [14:08<00:00, 47.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-26 16:11:38 14m9s \u001b[34mMETRIC  \u001b[0m \u001b[36marmory.instrument.instrument\u001b[0m:\u001b[36m_write\u001b[0m:\u001b[36m743\u001b[0m benign_mean_categorical_accuracy on benign examples w.r.t. ground truth labels: 0.961\n",
      "2023-07-26 16:11:38 14m9s \u001b[34mMETRIC  \u001b[0m \u001b[36marmory.instrument.instrument\u001b[0m:\u001b[36m_write\u001b[0m:\u001b[36m743\u001b[0m adversarial_mean_categorical_accuracy on adversarial examples w.r.t. ground truth labels: 0.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'armory_version': '23.4.0.post113+g0e7be67a.d20230713',\n",
       " 'evaluation': Evaluation(name='food-category-classification', description='Food category classification from HuggingFace', model=Model(name='food-category-classification', model=art.estimators.classification.pytorch.PyTorchClassifier(model=ModelWrapper(\n",
       "   (_model): HuggingFaceImageClassifier(\n",
       "     (model): SwinForImageClassification(\n",
       "       (swin): SwinModel(\n",
       "         (embeddings): SwinEmbeddings(\n",
       "           (patch_embeddings): SwinPatchEmbeddings(\n",
       "             (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "           )\n",
       "           (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "           (dropout): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (encoder): SwinEncoder(\n",
       "           (layers): ModuleList(\n",
       "             (0): SwinStage(\n",
       "               (blocks): ModuleList(\n",
       "                 (0-1): 2 x SwinLayer(\n",
       "                   (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                   (attention): SwinAttention(\n",
       "                     (self): SwinSelfAttention(\n",
       "                       (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                       (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                       (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                     (output): SwinSelfOutput(\n",
       "                       (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (drop_path): SwinDropPath(p=0.1)\n",
       "                   (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                   (intermediate): SwinIntermediate(\n",
       "                     (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                     (intermediate_act_fn): GELUActivation()\n",
       "                   )\n",
       "                   (output): SwinOutput(\n",
       "                     (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                     (dropout): Dropout(p=0.0, inplace=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (downsample): SwinPatchMerging(\n",
       "                 (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "                 (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "               )\n",
       "             )\n",
       "             (1): SwinStage(\n",
       "               (blocks): ModuleList(\n",
       "                 (0-1): 2 x SwinLayer(\n",
       "                   (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                   (attention): SwinAttention(\n",
       "                     (self): SwinSelfAttention(\n",
       "                       (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                       (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                       (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                     (output): SwinSelfOutput(\n",
       "                       (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (drop_path): SwinDropPath(p=0.1)\n",
       "                   (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                   (intermediate): SwinIntermediate(\n",
       "                     (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                     (intermediate_act_fn): GELUActivation()\n",
       "                   )\n",
       "                   (output): SwinOutput(\n",
       "                     (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                     (dropout): Dropout(p=0.0, inplace=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (downsample): SwinPatchMerging(\n",
       "                 (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "                 (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               )\n",
       "             )\n",
       "             (2): SwinStage(\n",
       "               (blocks): ModuleList(\n",
       "                 (0-17): 18 x SwinLayer(\n",
       "                   (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                   (attention): SwinAttention(\n",
       "                     (self): SwinSelfAttention(\n",
       "                       (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                       (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                       (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                     (output): SwinSelfOutput(\n",
       "                       (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (drop_path): SwinDropPath(p=0.1)\n",
       "                   (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                   (intermediate): SwinIntermediate(\n",
       "                     (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                     (intermediate_act_fn): GELUActivation()\n",
       "                   )\n",
       "                   (output): SwinOutput(\n",
       "                     (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                     (dropout): Dropout(p=0.0, inplace=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "               (downsample): SwinPatchMerging(\n",
       "                 (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                 (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "               )\n",
       "             )\n",
       "             (3): SwinStage(\n",
       "               (blocks): ModuleList(\n",
       "                 (0-1): 2 x SwinLayer(\n",
       "                   (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   (attention): SwinAttention(\n",
       "                     (self): SwinSelfAttention(\n",
       "                       (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                     (output): SwinSelfOutput(\n",
       "                       (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (drop_path): SwinDropPath(p=0.1)\n",
       "                   (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                   (intermediate): SwinIntermediate(\n",
       "                     (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                     (intermediate_act_fn): GELUActivation()\n",
       "                   )\n",
       "                   (output): SwinOutput(\n",
       "                     (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                     (dropout): Dropout(p=0.0, inplace=False)\n",
       "                   )\n",
       "                 )\n",
       "               )\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "       )\n",
       "       (classifier): Linear(in_features=1024, out_features=12, bias=True)\n",
       "     )\n",
       "   )\n",
       " ), loss=CrossEntropyLoss(), optimizer=Adam (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     differentiable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     fused: None\n",
       "     lr: 0.003\n",
       "     maximize: False\n",
       "     weight_decay: 0\n",
       " ), input_shape=(224, 224, 3), nb_classes=12, channels_first=False, clip_values=array([0., 1.], dtype=float32), preprocessing_defences=None, postprocessing_defences=None, preprocessing=StandardisationMeanStdPyTorch(mean=0.0, std=1.0, apply_fit=True, apply_predict=True, device=cuda:0)), predict_kwargs={}), scenario=Scenario(function=<class 'charmory.scenarios.image_classification.ImageClassificationTask'>, kwargs={}, export_batches=False), dataset=Dataset(name='food-category-classification', test_dataset=<charmory.data.JaticVisionDatasetGenerator object at 0x7fd2d8245510>, train_dataset=None), author='Kaludi', attack=Attack(function=<class 'art.attacks.evasion.projected_gradient_descent.projected_gradient_descent.ProjectedGradientDescent'>, kwargs={'batch_size': 1, 'eps': 0.031, 'eps_step': 0.007, 'max_iter': 20, 'num_random_init': 1, 'random_eps': False, 'targeted': False, 'verbose': False}, knowledge='white', use_label=True, type=None, generate_kwargs={}, sweep_params={}, targeted=False, targeted_labels={}), metric=Metric(profiler_type='basic', supported_metrics=['accuracy'], perturbation=['linf'], task=['categorical_accuracy'], means=True, record_metric_per_sample=False), sysconfig=SysConfig(gpus=['all'], use_gpu=True)),\n",
       " 'results': {'benign_mean_categorical_accuracy': [0.9607142857142857],\n",
       "  'adversarial_mean_categorical_accuracy': [0.48928571428571427],\n",
       "  'perturbation_mean_linf': [0.03100001811981201],\n",
       "  'compute': {'Avg. CPU time (s) for 18 executions of Inference': 0.747857161944517,\n",
       "   'Avg. CPU time (s) for 18 executions of Attack': 44.90390925911096}},\n",
       " 'timestamp': 1690401449}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from charmory.engine import Engine\n",
    "\n",
    "engine = Engine(evaluation)\n",
    "results = engine.run()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
