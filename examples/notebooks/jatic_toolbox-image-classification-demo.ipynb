{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by loading the model from HuggingFace using the JATIC-toolbox. We also\n",
        "use `track_params` to have all function arguments recorded with MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kyle-treubig/Code/jatic/armory/examples/.venv/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import jatic_toolbox\n",
        "\n",
        "from charmory.track import track_params\n",
        "\n",
        "model = track_params(jatic_toolbox.load_model)(\n",
        "    provider=\"huggingface\",\n",
        "    model_name=\"Kaludi/food-category-classification-v2.0\",\n",
        "    task=\"image-classification\"\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model returns a `HuggingFaceProbs` object type, but ART expects the model output to be the `y` tensor. So we have to adapt the model to produce the correct output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.utils import adapt_jatic_image_classification_model_for_art\n",
        "\n",
        "adapt_jatic_image_classification_model_for_art(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then wrap it in an ART classifier to make it compatible with Armory/ART.\n",
        "Since we are instantiating a class, we use `track_init_params` to have the\n",
        "object initialization arguments logged with MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from art.estimators.classification import PyTorchClassifier\n",
        "import torch\n",
        "\n",
        "from charmory.track import track_init_params\n",
        "\n",
        "classifier = track_init_params(PyTorchClassifier)(\n",
        "    model,\n",
        "    loss=torch.nn.CrossEntropyLoss(),\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.003),\n",
        "    input_shape=(224, 224, 3),\n",
        "    channels_first=False,\n",
        "    nb_classes=12,\n",
        "    clip_values=(0.0, 1.0),\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we load the dataset from from HuggingFace using the JATIC-toolbox."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-24 15:55:55 23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1706\u001b[0m Found cached dataset imagefolder (/home/kyle-treubig/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
            "2023-08-24 15:55:55 23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1706\u001b[0m Loading cached processed dataset at /home/kyle-treubig/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-b82aa949443f821a.arrow\n"
          ]
        }
      ],
      "source": [
        "dataset = track_params(jatic_toolbox.load_dataset)(\n",
        "    provider=\"huggingface\",\n",
        "    dataset_name=\"Kaludi/food-category-classification-v2.0\",\n",
        "    task=\"image-classification\",\n",
        "    split=\"validation\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this dataset contains bad images that will result in errors during evaluation, we will apply a filter to the underlying HuggingFace dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length prior to filtering: 300\n",
            "2023-08-24 15:55:55 23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1706\u001b[0m Loading cached processed dataset at /home/kyle-treubig/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-20e62b43d7f7d11a.arrow\n",
            "Dataset length after filtering: 280\n"
          ]
        }
      ],
      "source": [
        "from transformers.image_utils import infer_channel_dimension_format\n",
        "import numpy as np\n",
        "\n",
        "def filter(sample):\n",
        "    try:\n",
        "        infer_channel_dimension_format(np.asarray(sample[\"image\"]))\n",
        "        return True\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        return False\n",
        "\n",
        "print(f\"Dataset length prior to filtering: {len(dataset)}\")\n",
        "dataset._dataset = dataset._dataset.filter(filter)\n",
        "print(f\"Dataset length after filtering: {len(dataset)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then prepare a transform for the data using the preprocessor that comes with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.utils import create_jatic_dataset_transform\n",
        "\n",
        "transform = create_jatic_dataset_transform(model.preprocessor)\n",
        "dataset.set_transform(transform)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create an Armory data generator around the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-24 15:55:59 27s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1706\u001b[0m `tfds.core.add_checksums_dir` is deprecated. Refactor dataset in self-contained folders (`my_dataset/` folder containing my_dataset.py, my_dataset_test.py, dummy_data/, checksums.tsv). The checksum file will be automatically detected. More info at: https://www.tensorflow.org/datasets/add_dataset\n"
          ]
        }
      ],
      "source": [
        "from charmory.data import JaticVisionDatasetGenerator\n",
        "\n",
        "generator = JaticVisionDatasetGenerator(\n",
        "    dataset=dataset,\n",
        "    batch_size=16,\n",
        "    epochs=1,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly we will define the Armory evaluation, including the attack and scenario to be run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art.attacks.evasion\n",
        "from armory.instrument.config import MetricsLogger\n",
        "from armory.metrics.compute import BasicProfiler\n",
        "from charmory.evaluation import (\n",
        "    Attack,\n",
        "    Dataset,\n",
        "    Evaluation,\n",
        "    Metric,\n",
        "    Model,\n",
        "    Scenario,\n",
        "    SysConfig,\n",
        ")\n",
        "import charmory.scenarios.image_classification\n",
        "\n",
        "eval_dataset = Dataset(\n",
        "    name=\"food-category-classification\",\n",
        "    test_dataset=generator,\n",
        ")\n",
        "\n",
        "eval_model = Model(\n",
        "    name=\"food-category-classification\",\n",
        "    model=classifier,\n",
        ")\n",
        "\n",
        "eval_attack = Attack(\n",
        "    name=\"PGD\",\n",
        "    attack=track_init_params(art.attacks.evasion.ProjectedGradientDescent)(\n",
        "        classifier,\n",
        "        batch_size=1,\n",
        "        eps=0.031,\n",
        "        eps_step=0.007,\n",
        "        max_iter=20,\n",
        "        num_random_init=1,\n",
        "        random_eps=False,\n",
        "        targeted=False,\n",
        "        verbose=False,\n",
        "    ),\n",
        "    use_label_for_untargeted=True,\n",
        ")\n",
        "\n",
        "eval_scenario = Scenario(\n",
        "    function=charmory.scenarios.image_classification.ImageClassificationTask,\n",
        "    kwargs={},\n",
        ")\n",
        "\n",
        "eval_metric = Metric(\n",
        "    profiler=BasicProfiler(),\n",
        "    logger=MetricsLogger(\n",
        "        supported_metrics=[\"accuracy\"],\n",
        "        perturbation=[\"linf\"],\n",
        "        task=[\"categorical_accuracy\"],\n",
        "        means=True,\n",
        "        record_metric_per_sample=False,\n",
        "    ),\n",
        ")\n",
        "\n",
        "eval_sysconfig = SysConfig(\n",
        "    gpus=[\"all\"],\n",
        "    use_gpu=True,\n",
        ")\n",
        "\n",
        "evaluation = Evaluation(\n",
        "    name=\"jatic-food-category-classification\",\n",
        "    description=\"Food category classification from HuggingFace via JATIC-toolbox\",\n",
        "    author=\"Kaludi\",\n",
        "    dataset=eval_dataset,\n",
        "    model=eval_model,\n",
        "    attack=eval_attack,\n",
        "    scenario=eval_scenario,\n",
        "    metric=eval_metric,\n",
        "    sysconfig=eval_sysconfig,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create an engine for the evaluation and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|██████████| 18/18 [15:09<00:00, 50.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-24 16:11:09 15m37s \u001b[34mMETRIC  \u001b[0m \u001b[36marmory.instrument.instrument\u001b[0m:\u001b[36m_write\u001b[0m:\u001b[36m742\u001b[0m benign_mean_categorical_accuracy on benign examples w.r.t. ground truth labels: 0.961\n",
            "2023-08-24 16:11:09 15m37s \u001b[34mMETRIC  \u001b[0m \u001b[36marmory.instrument.instrument\u001b[0m:\u001b[36m_write\u001b[0m:\u001b[36m742\u001b[0m adversarial_mean_categorical_accuracy on adversarial examples w.r.t. ground truth labels: 0.479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'armory_version': '23.7.2.post132+gb3a8c3e5.d20230817',\n",
              " 'evaluation': Evaluation(name='jatic-food-category-classification', description='Food category classification from HuggingFace via JATIC-toolbox', model=Model(name='food-category-classification', model=art.estimators.classification.pytorch.PyTorchClassifier(model=ModelWrapper(\n",
              "   (_model): HuggingFaceImageClassifier(\n",
              "     (model): SwinForImageClassification(\n",
              "       (swin): SwinModel(\n",
              "         (embeddings): SwinEmbeddings(\n",
              "           (patch_embeddings): SwinPatchEmbeddings(\n",
              "             (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
              "           )\n",
              "           (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout): Dropout(p=0.0, inplace=False)\n",
              "         )\n",
              "         (encoder): SwinEncoder(\n",
              "           (layers): ModuleList(\n",
              "             (0): SwinStage(\n",
              "               (blocks): ModuleList(\n",
              "                 (0-1): 2 x SwinLayer(\n",
              "                   (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "                   (attention): SwinAttention(\n",
              "                     (self): SwinSelfAttention(\n",
              "                       (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                       (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                       (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                     (output): SwinSelfOutput(\n",
              "                       (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                   )\n",
              "                   (drop_path): SwinDropPath(p=0.1)\n",
              "                   (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "                   (intermediate): SwinIntermediate(\n",
              "                     (dense): Linear(in_features=128, out_features=512, bias=True)\n",
              "                     (intermediate_act_fn): GELUActivation()\n",
              "                   )\n",
              "                   (output): SwinOutput(\n",
              "                     (dense): Linear(in_features=512, out_features=128, bias=True)\n",
              "                     (dropout): Dropout(p=0.0, inplace=False)\n",
              "                   )\n",
              "                 )\n",
              "               )\n",
              "               (downsample): SwinPatchMerging(\n",
              "                 (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
              "                 (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "               )\n",
              "             )\n",
              "             (1): SwinStage(\n",
              "               (blocks): ModuleList(\n",
              "                 (0-1): 2 x SwinLayer(\n",
              "                   (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "                   (attention): SwinAttention(\n",
              "                     (self): SwinSelfAttention(\n",
              "                       (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "                       (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "                       (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                     (output): SwinSelfOutput(\n",
              "                       (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                   )\n",
              "                   (drop_path): SwinDropPath(p=0.1)\n",
              "                   (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "                   (intermediate): SwinIntermediate(\n",
              "                     (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                     (intermediate_act_fn): GELUActivation()\n",
              "                   )\n",
              "                   (output): SwinOutput(\n",
              "                     (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                     (dropout): Dropout(p=0.0, inplace=False)\n",
              "                   )\n",
              "                 )\n",
              "               )\n",
              "               (downsample): SwinPatchMerging(\n",
              "                 (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                 (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "               )\n",
              "             )\n",
              "             (2): SwinStage(\n",
              "               (blocks): ModuleList(\n",
              "                 (0-17): 18 x SwinLayer(\n",
              "                   (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "                   (attention): SwinAttention(\n",
              "                     (self): SwinSelfAttention(\n",
              "                       (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "                       (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "                       (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                     (output): SwinSelfOutput(\n",
              "                       (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                   )\n",
              "                   (drop_path): SwinDropPath(p=0.1)\n",
              "                   (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "                   (intermediate): SwinIntermediate(\n",
              "                     (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "                     (intermediate_act_fn): GELUActivation()\n",
              "                   )\n",
              "                   (output): SwinOutput(\n",
              "                     (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                     (dropout): Dropout(p=0.0, inplace=False)\n",
              "                   )\n",
              "                 )\n",
              "               )\n",
              "               (downsample): SwinPatchMerging(\n",
              "                 (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "                 (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "               )\n",
              "             )\n",
              "             (3): SwinStage(\n",
              "               (blocks): ModuleList(\n",
              "                 (0-1): 2 x SwinLayer(\n",
              "                   (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                   (attention): SwinAttention(\n",
              "                     (self): SwinSelfAttention(\n",
              "                       (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                       (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                       (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                     (output): SwinSelfOutput(\n",
              "                       (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                   )\n",
              "                   (drop_path): SwinDropPath(p=0.1)\n",
              "                   (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                   (intermediate): SwinIntermediate(\n",
              "                     (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "                     (intermediate_act_fn): GELUActivation()\n",
              "                   )\n",
              "                   (output): SwinOutput(\n",
              "                     (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "                     (dropout): Dropout(p=0.0, inplace=False)\n",
              "                   )\n",
              "                 )\n",
              "               )\n",
              "             )\n",
              "           )\n",
              "         )\n",
              "         (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "         (pooler): AdaptiveAvgPool1d(output_size=1)\n",
              "       )\n",
              "       (classifier): Linear(in_features=1024, out_features=12, bias=True)\n",
              "     )\n",
              "   )\n",
              " ), loss=CrossEntropyLoss(), optimizer=Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     capturable: False\n",
              "     differentiable: False\n",
              "     eps: 1e-08\n",
              "     foreach: None\n",
              "     fused: None\n",
              "     lr: 0.003\n",
              "     maximize: False\n",
              "     weight_decay: 0\n",
              " ), input_shape=(224, 224, 3), nb_classes=12, channels_first=False, clip_values=array([0., 1.], dtype=float32), preprocessing_defences=None, postprocessing_defences=None, preprocessing=StandardisationMeanStdPyTorch(mean=0.0, std=1.0, apply_fit=True, apply_predict=True, device=cuda:0)), predict_kwargs={}), scenario=Scenario(function=<class 'charmory.scenarios.image_classification.ImageClassificationTask'>, kwargs={}, export_batches=False), dataset=Dataset(name='food-category-classification', test_dataset=<charmory.data.JaticVisionDatasetGenerator object at 0x7f5b5442cf10>, train_dataset=None), author='Kaludi', attack=Attack(name='PGD', attack=<art.attacks.evasion.projected_gradient_descent.projected_gradient_descent.ProjectedGradientDescent object at 0x7f5ab4d04390>, generate_kwargs={}, use_label_for_untargeted=True, label_targeter=None), metric=Metric(profiler_type='basic', supported_metrics=['accuracy'], perturbation=['linf'], task=['categorical_accuracy'], means=True, record_metric_per_sample=False), sysconfig=SysConfig(gpus=['all'], use_gpu=True, paths={'armory_home': PosixPath('/home/kyle-treubig/.armory'), 'dataset_dir': PosixPath('/home/kyle-treubig/.armory/datasets'), 'saved_model_dir': PosixPath('/home/kyle-treubig/.armory/saved_models'), 'output_dir': PosixPath('/home/kyle-treubig/.armory/outputs'), 'external_repo_dir': PosixPath('/home/kyle-treubig/.armory/tmp/external')}, armory_home=PosixPath('/home/kyle-treubig/.armory'))),\n",
              " 'results': {'metrics': {'benign_mean_categorical_accuracy': [0.9607142857142857],\n",
              "   'adversarial_mean_categorical_accuracy': [0.4785714285714286],\n",
              "   'perturbation_mean_linf': [0.03100001811981201]},\n",
              "  'compute': {'Avg. CPU time (s) for 18 executions of Inference': 0.7913730249446316,\n",
              "   'Avg. CPU time (s) for 18 executions of Attack': 48.15218202233336}},\n",
              " 'timestamp': 1692906960}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from charmory.engine import Engine\n",
        "\n",
        "engine = Engine(evaluation)\n",
        "results = engine.run()\n",
        "results"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
