{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ef4a1101c507e5",
   "metadata": {},
   "source": [
    "# New Object Detection Dataset\n",
    "\n",
    "This notebook describes the process of preparing a user-provided object detection dataset (one not included in Hugging Face or Torchvision) for use in Armory Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Counter, Dict, Iterator, List, Tuple\n",
    "\n",
    "import datasets\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "import armory.data\n",
    "import armory.dataset\n",
    "import armory.evaluation\n",
    "import armory.examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5960156d72e13f35",
   "metadata": {},
   "source": [
    "## [VisDrone 2019 Dataset](https://github.com/VisDrone/VisDrone-Dataset)\n",
    "\n",
    "Drones equipped with cameras are increasingly used for a variety of applications such as agriculture, aerial photography delivery, and surveillance necessitating advanced automatic visual data analysis through computer vision. The VisDrone2019 dataset, created by the AISKYEYE team at Tianjin University, China, includes 288 video clips and 10,209 images from various drones, providing a comprehensive benchmark with over 2.6 million manually annotated bounding boxes for objects like pedestrians and vehicles across diverse conditions and locations.\n",
    "\n",
    "We will import the data for the object detection Task 1 challenge. This task aims to detect objects of the predefined categories described above from individual images taken from drones. Google Drive links to the training, validation and test splits are provided in the repository [README](https://github.com/VisDrone/VisDrone-Dataset/blob/master/README.md).\n",
    "\n",
    "The schema and semantics of the image annotation files are defined in the [VisDrone Toolkit](https://github.com/VisDrone/VisDrone2018-DET-toolkit/blob/master/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb17ed3530c785",
   "metadata": {},
   "source": [
    "## Download dataset\n",
    "\n",
    "As a first step, we download the [validation split](https://drive.google.com/file/d/1bxK5zgLn0_L8x276eKkuYA_FzwCIjb59/view?usp=sharing) to a temporary directory. Note that we do not need to unzip the archive for processing as a Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c21e2a012934081",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dir = Path('/tmp')\n",
    "visdrone_dir = tmp_dir / Path('visdrone_2019')\n",
    "visdrone_dir.mkdir(exist_ok=True)\n",
    "\n",
    "visdrone_val_zip = visdrone_dir / Path('VisDrone2019-DET-val.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac127302d65c3",
   "metadata": {},
   "source": [
    "### Dataset structure\n",
    "\n",
    "The VisDrone 2019 Task 1 dataset is organized as parallel folders of `images` and `annotations` containing pairs of image and annotation files, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3bbd8ff06a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -l $visdrone_val_zip '*/0000023_*.*'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4c00f218d52c0",
   "metadata": {},
   "source": [
    "The annotation files contain object descriptions for each paired image. For each object, one per line, the annotation file includes the bounding box coordinates, the bounding box score, the object category, and truncation and occlusion flags.\n",
    "\n",
    "The object category indicates the type of annotated object below.\n",
    "  - ignored regions(0)\n",
    "  - pedestrian(1)\n",
    "  - people(2)\n",
    "  - bicycle(3)\n",
    "  - car(4)\n",
    "  - van(5)\n",
    "  - truck(6)\n",
    "  - tricycle(7)\n",
    "  - awning-tricycle(8)\n",
    "  - bus(9)\n",
    "  - motor(10)\n",
    "  - others(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f66622e6e7b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -p $visdrone_val_zip 'VisDrone2019-DET-val/annotations/0000023_00868_d_0000010.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8bc4bae925499",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "As a preliminary, we designate the object categories and name the fields in the annotation files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0960c6a5acf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\n",
    "    'ignored',\n",
    "    'pedestrian',\n",
    "    'people',\n",
    "    'bicycle',\n",
    "    'car',\n",
    "    'van',\n",
    "    'truck',\n",
    "    'tricycle',\n",
    "    'awning-tricycle',\n",
    "    'bus',\n",
    "    'motor',\n",
    "    'other'\n",
    "]\n",
    "\n",
    "ANNOTATION_FIELDS = [\n",
    "    'x',\n",
    "    'y',\n",
    "    'width',\n",
    "    'height',\n",
    "    'score',\n",
    "    'category_id',\n",
    "    'truncation',\n",
    "    'occlusion'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c692632829d04bf",
   "metadata": {},
   "source": [
    "Next, we define the possibly hierarchical features of the dataset by instantiating a [`datasets.Features`](https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/main_classes#datasets.Features) object -- each feature is named and a Hugging Face data type provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98017999d8134b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = datasets.Features(\n",
    "    {\n",
    "        'image_id': datasets.Value('int64'),\n",
    "        'file_name': datasets.Value('string'),\n",
    "        'image': datasets.Image(),\n",
    "        'objects': datasets.Sequence(\n",
    "            {\n",
    "                'id': datasets.Value('int64'),\n",
    "                'bbox': datasets.Sequence(datasets.Value('float32'), length=4),\n",
    "                'category': datasets.ClassLabel(num_classes=len(CATEGORIES), names=CATEGORIES),\n",
    "                'truncation': datasets.Value('int32'),\n",
    "                'occlusion': datasets.Value('int32'),\n",
    "            }\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2cc8788567758",
   "metadata": {},
   "source": [
    "### Loading functions\n",
    "\n",
    "To load the VisDrone 2019 dataset, we define functions `load_annotations` and `generate_examples`. The `load_annotation` function take a reader for an annotation file, parses each object description into a dictionary and returns a list of object descriptors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4808924150c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(f: io.BufferedReader) -> List[Dict]:\n",
    "        reader = csv.DictReader(io.StringIO(f.read().decode('utf-8')), fieldnames=ANNOTATION_FIELDS)\n",
    "        annotations = []\n",
    "        for idx, row in enumerate(reader):\n",
    "            category_id = int(row['category_id'])\n",
    "            annotation = {\n",
    "                'id': idx,\n",
    "                'bbox': list(map(float, [row[k] for k in ANNOTATION_FIELDS[:4]])),\n",
    "                'category': category_id,\n",
    "                'truncation': row['truncation'],\n",
    "                'occlusion': row['occlusion']\n",
    "            }\n",
    "            annotations.append(annotation)\n",
    "        return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc12859ad9a19499",
   "metadata": {},
   "source": [
    "The `generate_examples` generator is called by Huffing Face to produce the examples in the split being loaded. Note that in this example, the `files` parameter is a reader attached to the VisDrone split archive. The generator assumes that annotation files are read first, and then pairs image files with the corresponding annotations to yield rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b398dd6c9a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(files: Iterator[Tuple[str, io.BufferedReader]], annotation_file_ext:str ='.txt') -> Iterator[Dict[str, object]]:\n",
    "    annotations = {}\n",
    "    images = {}\n",
    "    for path, f in files:\n",
    "        file_name, _ = os.path.splitext(os.path.basename(path))\n",
    "        if path.endswith(annotation_file_ext):\n",
    "            annotation = load_annotations(f)\n",
    "            annotations[file_name] = annotation\n",
    "        else:\n",
    "            images[file_name] = {'path': path, 'bytes': f.read()}\n",
    "    for idx, (file_name, annotation) in enumerate(annotations.items()):\n",
    "        example = {\n",
    "            'image_id': idx,\n",
    "            'file_name': file_name,\n",
    "            'image': images[file_name],\n",
    "            'objects': annotation,\n",
    "        }\n",
    "        yield example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaa0e8415f590b5",
   "metadata": {},
   "source": [
    "Now we create the validation dataset from the split archive by calling [`datasets.Dataset.from_generator`](https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/main_classes#datasets.Dataset.from_generator). Creating a dataset from a generator function is the preferred method, but other options are available (e.g. Pandas dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04f36e894d4193",
   "metadata": {},
   "outputs": [],
   "source": [
    "visdrone_val_files = datasets.DownloadManager().iter_archive(visdrone_val_zip)\n",
    "\n",
    "visdrone_dataset = datasets.Dataset.from_generator(\n",
    "    generate_examples,\n",
    "    gen_kwargs={\n",
    "    \"files\": visdrone_val_files,\n",
    "    },\n",
    "    features=features,\n",
    "    \n",
    ")\n",
    "visdrone_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39473b0b047c4bb",
   "metadata": {},
   "source": [
    "### Verify dataset\n",
    "\n",
    "Check that image features, particularly the category names, have been defined correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dd715fa2ed995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visdrone_labels: List[str] = visdrone_dataset.features['objects'].feature['category'].names\n",
    "visdrone_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca65ed1199ddaf31",
   "metadata": {},
   "source": [
    "Display a sample image with annotation boxes and category labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce35f1a3fe83fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = visdrone_dataset[1]\n",
    "\n",
    "boxes_xywh = torch.tensor(example['objects']['bbox'])\n",
    "boxes_xyxy = torchvision.ops.box_convert(boxes_xywh, 'xywh', 'xyxy')\n",
    "\n",
    "categories = visdrone_dataset.features['objects'].feature['category'].names\n",
    "labels = [categories[x] for x in example['objects']['category']]\n",
    "\n",
    "image = F.to_pil_image(\n",
    "    torchvision.utils.draw_bounding_boxes(\n",
    "        F.pil_to_tensor(example['image']),\n",
    "        boxes_xyxy,\n",
    "        colors=\"red\",\n",
    "        labels=labels,\n",
    "    )\n",
    ")\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45d557ee6ada8fc",
   "metadata": {},
   "source": [
    "### Dataset statistics\n",
    "\n",
    "Using the Hugging Face [`map`](https://huggingface.co/docs/datasets/v2.19.0/en/image_process#map) function that can apply a transform over an entire dataset, we can produce simple statistics that summarize the data. For example, the `count_labels` function accumulates counts of the number of objects of each category that are then used to create a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb015a4c4df0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(ds: datasets.Dataset) -> List[int]:\n",
    "    ctr: Counter[str] = collections.Counter()\n",
    "    \n",
    "    def inc_labels(objects:dict) -> None:\n",
    "        for l in objects['category']:\n",
    "            ctr[visdrone_labels[l]] += 1\n",
    "            \n",
    "    ds.map(inc_labels, input_columns=['objects'])\n",
    "    counts = [ctr[l] for l in visdrone_labels]\n",
    "    return counts\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'valid': count_labels(visdrone_dataset)},\n",
    "    index=visdrone_labels\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca62ab797fe6a3a",
   "metadata": {},
   "source": [
    "A bar chart of the category counts clearly reveals the imbalance of the VisDrone 2019 validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c987c930ec09a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar', \n",
    "        title='VisDrone Class Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e58d7eb97e8f5b",
   "metadata": {},
   "source": [
    "### Saving to Disk or Uploading to S3 Bucket\n",
    "\n",
    "The new VisDrone dataset may be [saved to disk](https://huggingface.co/docs/datasets/v2.19.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk) among other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9756f7faf5578f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visdrone_path = Path('visdrone_2019.hf')    \n",
    "visdrone_dataset.save_to_disk(visdrone_path)\n",
    "    \n",
    "print(\"Loading the dataset\")\n",
    "print(datasets.load_from_disk(visdrone_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36663ae8b77aff",
   "metadata": {},
   "source": [
    "## Integrate into Armory\n",
    "\n",
    "Having imported VisDrone 2019 as a Hugging Face dataset, we are ready to plug our new dataset into the Armory Library framework. This consists of creating an `armory.dataset.ObjectDetectionDataLoader` that defines the underlying [PyTorch dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). Note that the `armory.data.Scale` object defines the type and scale of the data. The Armory dataloader is then wrapped by an `armory.evaluation.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579cd100bfc9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "shuffle = False\n",
    "\n",
    "unnormalized_scale = armory.data.Scale(\n",
    "    dtype=armory.data.DataType.UINT8,\n",
    "    max=255,\n",
    ")\n",
    "\n",
    "mstar_dataloader = armory.dataset.ObjectDetectionDataLoader(\n",
    "    visdrone_dataset,\n",
    "    image_key='image',\n",
    "    dim=armory.data.ImageDimensions.CHW,\n",
    "    scale=unnormalized_scale,\n",
    "    objects_key='objects',\n",
    "    boxes_key='bbox',\n",
    "    format=armory.data.BBoxFormat.XYWH,\n",
    "    labels_key='category',\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    ")\n",
    "\n",
    "armory_dataset = armory.evaluation.Dataset(\n",
    "    name=\"visdrone-2019\",\n",
    "    dataloader=mstar_dataloader,\n",
    ")\n",
    "\n",
    "armory_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ba73b32bd0f5f",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- [VisDrone 2019 Dataset](https://github.com/VisDrone/VisDrone-Dataset)\n",
    "\n",
    "- [VisDrone Toolkit](https://github.com/VisDrone/VisDrone2018-DET-toolkit/blob/master/README.md)\n",
    "\n",
    "- [Zhu, P., Wen, L., Du, D., Bian, X., Fan, H., Hu, Q., & Ling, H. (2021). Detection and tracking meet drones challenge. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11), 7380-7399.](https://arxiv.org/abs/2001.06303)\n",
    "\n",
    "- [Hugging Face Datasets](https://huggingface.co/docs/datasets/index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "armory-library-venv",
   "language": "python",
   "name": "armory-library-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
