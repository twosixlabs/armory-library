{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by loading the model from HuggingFace using the JATIC-toolbox. We also\n",
        "use `track_params` to have all function arguments recorded with MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/kyle-treubig/Code/jatic/armory/examples/.venv/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import jatic_toolbox\n",
        "\n",
        "from charmory.track import track_params\n",
        "\n",
        "model = track_params(jatic_toolbox.load_model)(\n",
        "    provider=\"huggingface\",\n",
        "    model_name=\"Kaludi/food-category-classification-v2.0\",\n",
        "    task=\"image-classification\"\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model returns a `HuggingFaceProbs` object type, but ART expects the model output to be the `y` tensor. So we have to adapt the model to produce the correct output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.utils import adapt_jatic_image_classification_model_for_art\n",
        "\n",
        "adapt_jatic_image_classification_model_for_art(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then wrap it in an ART classifier to make it compatible with Armory/ART.\n",
        "Since we are instantiating a class, we use `track_init_params` to have the\n",
        "object initialization arguments logged with MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from art.estimators.classification import PyTorchClassifier\n",
        "import torch\n",
        "\n",
        "from charmory.track import track_init_params\n",
        "\n",
        "classifier = track_init_params(PyTorchClassifier)(\n",
        "    model,\n",
        "    loss=torch.nn.CrossEntropyLoss(),\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.003),\n",
        "    input_shape=(224, 224, 3),\n",
        "    channels_first=False,\n",
        "    nb_classes=12,\n",
        "    clip_values=(0.0, 1.0),\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we load the dataset from from HuggingFace using the JATIC-toolbox."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-29 10:01:09 17s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1706\u001b[0m Found cached dataset imagefolder (/home/kyle-treubig/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
            "2023-08-29 10:01:09 18s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1706\u001b[0m Loading cached processed dataset at /home/kyle-treubig/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-b82aa949443f821a.arrow\n"
          ]
        }
      ],
      "source": [
        "dataset = track_params(jatic_toolbox.load_dataset)(\n",
        "    provider=\"huggingface\",\n",
        "    dataset_name=\"Kaludi/food-category-classification-v2.0\",\n",
        "    task=\"image-classification\",\n",
        "    split=\"validation\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this dataset contains bad images that will result in errors during evaluation, we will apply a filter to the underlying HuggingFace dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length prior to filtering: 300\n",
            "2023-08-29 10:01:09 18s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1706\u001b[0m Loading cached processed dataset at /home/kyle-treubig/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-20e62b43d7f7d11a.arrow\n",
            "Dataset length after filtering: 280\n"
          ]
        }
      ],
      "source": [
        "from transformers.image_utils import infer_channel_dimension_format\n",
        "import numpy as np\n",
        "\n",
        "def filter(sample):\n",
        "    try:\n",
        "        infer_channel_dimension_format(np.asarray(sample[\"image\"]))\n",
        "        return True\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        return False\n",
        "\n",
        "print(f\"Dataset length prior to filtering: {len(dataset)}\")\n",
        "dataset._dataset = dataset._dataset.filter(filter)\n",
        "print(f\"Dataset length after filtering: {len(dataset)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then prepare a transform for the data using the preprocessor that comes with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.utils import create_jatic_image_classification_dataset_transform\n",
        "\n",
        "transform = create_jatic_image_classification_dataset_transform(model.preprocessor)\n",
        "dataset.set_transform(transform)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create an Armory data generator around the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-29 10:01:13 22s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mlogging\u001b[0m:\u001b[36mcallHandlers\u001b[0m:\u001b[36m1706\u001b[0m `tfds.core.add_checksums_dir` is deprecated. Refactor dataset in self-contained folders (`my_dataset/` folder containing my_dataset.py, my_dataset_test.py, dummy_data/, checksums.tsv). The checksum file will be automatically detected. More info at: https://www.tensorflow.org/datasets/add_dataset\n"
          ]
        }
      ],
      "source": [
        "from charmory.data import JaticVisionDatasetGenerator\n",
        "\n",
        "generator = JaticVisionDatasetGenerator(\n",
        "    dataset=dataset,\n",
        "    batch_size=16,\n",
        "    epochs=1,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly we will define the Armory evaluation, including the attack and scenario to be run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art.attacks.evasion\n",
        "from armory.instrument.config import MetricsLogger\n",
        "from armory.metrics.compute import BasicProfiler\n",
        "from charmory.evaluation import (\n",
        "    Attack,\n",
        "    Dataset,\n",
        "    Evaluation,\n",
        "    Metric,\n",
        "    Model,\n",
        "    Scenario,\n",
        "    SysConfig,\n",
        ")\n",
        "import charmory.scenarios.image_classification\n",
        "\n",
        "def make_evaluation(epsilon: float) -> Evaluation:\n",
        "\n",
        "    eval_dataset = Dataset(\n",
        "        name=\"food-category-classification\",\n",
        "        test_dataset=generator,\n",
        "    )\n",
        "\n",
        "    eval_model = Model(\n",
        "        name=\"food-category-classification\",\n",
        "        model=classifier,\n",
        "    )\n",
        "\n",
        "    eval_attack = Attack(\n",
        "        name=\"PGD\",\n",
        "        attack=track_init_params(art.attacks.evasion.ProjectedGradientDescent)(\n",
        "            classifier,\n",
        "            batch_size=1,\n",
        "            eps=epsilon,\n",
        "            eps_step=0.007,\n",
        "            max_iter=20,\n",
        "            num_random_init=1,\n",
        "            random_eps=False,\n",
        "            targeted=False,\n",
        "            verbose=False,\n",
        "        ),\n",
        "        use_label_for_untargeted=True,\n",
        "    )\n",
        "\n",
        "    eval_scenario = Scenario(\n",
        "        function=charmory.scenarios.image_classification.ImageClassificationTask,\n",
        "        kwargs={},\n",
        "    )\n",
        "\n",
        "    eval_metric = Metric(\n",
        "        profiler=BasicProfiler(),\n",
        "        logger=MetricsLogger(\n",
        "            supported_metrics=[\"accuracy\"],\n",
        "            perturbation=[\"linf\"],\n",
        "            task=[\"categorical_accuracy\"],\n",
        "            means=True,\n",
        "            record_metric_per_sample=False,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    eval_sysconfig = SysConfig(\n",
        "        gpus=[\"all\"],\n",
        "        use_gpu=True,\n",
        "    )\n",
        "\n",
        "    return Evaluation(\n",
        "        name=\"jatic-food-demo\",\n",
        "        description=\"Tracked food category classification from HuggingFace via JATIC-toolbox\",\n",
        "        author=\"Kaludi\",\n",
        "        dataset=eval_dataset,\n",
        "        model=eval_model,\n",
        "        attack=eval_attack,\n",
        "        scenario=eval_scenario,\n",
        "        metric=eval_metric,\n",
        "        sysconfig=eval_sysconfig,\n",
        "    )\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create an engine for the evaluation and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|██████████| 18/18 [12:09<00:00, 40.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-29 10:13:23 12m31s \u001b[34mMETRIC  \u001b[0m \u001b[36marmory.instrument.instrument\u001b[0m:\u001b[36m_write\u001b[0m:\u001b[36m739\u001b[0m benign_mean_categorical_accuracy on benign examples w.r.t. ground truth labels: 0.961\n",
            "2023-08-29 10:13:23 12m31s \u001b[34mMETRIC  \u001b[0m \u001b[36marmory.instrument.instrument\u001b[0m:\u001b[36m_write\u001b[0m:\u001b[36m739\u001b[0m adversarial_mean_categorical_accuracy on adversarial examples w.r.t. ground truth labels: 0.864\n",
            "2023-08-29 10:13:23 12m31s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.instrument\u001b[0m:\u001b[36mfinalize\u001b[0m:\u001b[36m544\u001b[0m Meter 'targeted_categorical_accuracy' was never measured. The following args were never set: ['scenario.y_target']\n",
            "{'armory_version': '23.8.post51+gd9f4db7c.d20230829',\n",
            " 'evaluation': Evaluation(name='jatic-food-demo',\n",
            "                          description='Tracked food category classification '\n",
            "                                      'from HuggingFace via JATIC-toolbox',\n",
            "                          model=Model(name='food-category-classification',\n",
            "                                      model=art.estimators.classification.pytorch.PyTorchClassifier(model=ModelWrapper(\n",
            "  (_model): HuggingFaceImageClassifier(\n",
            "    (model): SwinForImageClassification(\n",
            "      (swin): SwinModel(\n",
            "        (embeddings): SwinEmbeddings(\n",
            "          (patch_embeddings): SwinPatchEmbeddings(\n",
            "            (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
            "          )\n",
            "          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (encoder): SwinEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0): SwinStage(\n",
            "              (blocks): ModuleList(\n",
            "                (0-1): 2 x SwinLayer(\n",
            "                  (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attention): SwinAttention(\n",
            "                    (self): SwinSelfAttention(\n",
            "                      (query): Linear(in_features=128, out_features=128, bias=True)\n",
            "                      (key): Linear(in_features=128, out_features=128, bias=True)\n",
            "                      (value): Linear(in_features=128, out_features=128, bias=True)\n",
            "                      (dropout): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                    (output): SwinSelfOutput(\n",
            "                      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
            "                      (dropout): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (drop_path): SwinDropPath(p=0.1)\n",
            "                  (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "                  (intermediate): SwinIntermediate(\n",
            "                    (dense): Linear(in_features=128, out_features=512, bias=True)\n",
            "                    (intermediate_act_fn): GELUActivation()\n",
            "                  )\n",
            "                  (output): SwinOutput(\n",
            "                    (dense): Linear(in_features=512, out_features=128, bias=True)\n",
            "                    (dropout): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (downsample): SwinPatchMerging(\n",
            "                (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
            "                (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "            )\n",
            "            (1): SwinStage(\n",
            "              (blocks): ModuleList(\n",
            "                (0-1): 2 x SwinLayer(\n",
            "                  (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attention): SwinAttention(\n",
            "                    (self): SwinSelfAttention(\n",
            "                      (query): Linear(in_features=256, out_features=256, bias=True)\n",
            "                      (key): Linear(in_features=256, out_features=256, bias=True)\n",
            "                      (value): Linear(in_features=256, out_features=256, bias=True)\n",
            "                      (dropout): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                    (output): SwinSelfOutput(\n",
            "                      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "                      (dropout): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (drop_path): SwinDropPath(p=0.1)\n",
            "                  (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "                  (intermediate): SwinIntermediate(\n",
            "                    (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
            "                    (intermediate_act_fn): GELUActivation()\n",
            "                  )\n",
            "                  (output): SwinOutput(\n",
            "                    (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
            "                    (dropout): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (downsample): SwinPatchMerging(\n",
            "                (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
            "                (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "            )\n",
            "            (2): SwinStage(\n",
            "              (blocks): ModuleList(\n",
            "                (0-17): 18 x SwinLayer(\n",
            "                  (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attention): SwinAttention(\n",
            "                    (self): SwinSelfAttention(\n",
            "                      (query): Linear(in_features=512, out_features=512, bias=True)\n",
            "                      (key): Linear(in_features=512, out_features=512, bias=True)\n",
            "                      (value): Linear(in_features=512, out_features=512, bias=True)\n",
            "                      (dropout): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                    (output): SwinSelfOutput(\n",
            "                      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
            "                      (dropout): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (drop_path): SwinDropPath(p=0.1)\n",
            "                  (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "                  (intermediate): SwinIntermediate(\n",
            "                    (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                    (intermediate_act_fn): GELUActivation()\n",
            "                  )\n",
            "                  (output): SwinOutput(\n",
            "                    (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
            "                    (dropout): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (downsample): SwinPatchMerging(\n",
            "                (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "                (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "            )\n",
            "            (3): SwinStage(\n",
            "              (blocks): ModuleList(\n",
            "                (0-1): 2 x SwinLayer(\n",
            "                  (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                  (attention): SwinAttention(\n",
            "                    (self): SwinSelfAttention(\n",
            "                      (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                      (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                      (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                      (dropout): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                    (output): SwinSelfOutput(\n",
            "                      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                      (dropout): Dropout(p=0.0, inplace=False)\n",
            "                    )\n",
            "                  )\n",
            "                  (drop_path): SwinDropPath(p=0.1)\n",
            "                  (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                  (intermediate): SwinIntermediate(\n",
            "                    (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                    (intermediate_act_fn): GELUActivation()\n",
            "                  )\n",
            "                  (output): SwinOutput(\n",
            "                    (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "                    (dropout): Dropout(p=0.0, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (pooler): AdaptiveAvgPool1d(output_size=1)\n",
            "      )\n",
            "      (classifier): Linear(in_features=1024, out_features=12, bias=True)\n",
            "    )\n",
            "  )\n",
            "), loss=CrossEntropyLoss(), optimizer=Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    capturable: False\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.003\n",
            "    maximize: False\n",
            "    weight_decay: 0\n",
            "), input_shape=(224, 224, 3), nb_classes=12, channels_first=False, clip_values=array([0., 1.], dtype=float32), preprocessing_defences=None, postprocessing_defences=None, preprocessing=StandardisationMeanStdPyTorch(mean=0.0, std=1.0, apply_fit=True, apply_predict=True, device=cuda:0)),\n",
            "                                      predict_kwargs={}),\n",
            "                          scenario=Scenario(function=<class 'charmory.scenarios.image_classification.ImageClassificationTask'>,\n",
            "                                            kwargs={},\n",
            "                                            export_batches=False),\n",
            "                          dataset=Dataset(name='food-category-classification',\n",
            "                                          test_dataset=<charmory.data.JaticVisionDatasetGenerator object at 0x7fd384595b10>,\n",
            "                                          train_dataset=None),\n",
            "                          author='Kaludi',\n",
            "                          attack=Attack(name='PGD',\n",
            "                                        attack=<art.attacks.evasion.projected_gradient_descent.projected_gradient_descent.ProjectedGradientDescent object at 0x7fd311c61250>,\n",
            "                                        generate_kwargs={},\n",
            "                                        use_label_for_untargeted=True,\n",
            "                                        label_targeter=None),\n",
            "                          metric=Metric(logger=<armory.instrument.config.MetricsLogger object at 0x7fd311c60590>,\n",
            "                                        profiler=<armory.metrics.compute.BasicProfiler object at 0x7fd311c60850>),\n",
            "                          sysconfig=SysConfig(gpus=['all'],\n",
            "                                              use_gpu=True,\n",
            "                                              paths={'armory_home': PosixPath('/home/kyle-treubig/.armory'),\n",
            "                                                     'dataset_dir': PosixPath('/home/kyle-treubig/.armory/datasets'),\n",
            "                                                     'external_repo_dir': PosixPath('/home/kyle-treubig/.armory/tmp/external'),\n",
            "                                                     'output_dir': PosixPath('/home/kyle-treubig/.armory/outputs'),\n",
            "                                                     'saved_model_dir': PosixPath('/home/kyle-treubig/.armory/saved_models')},\n",
            "                                              armory_home=PosixPath('/home/kyle-treubig/.armory'))),\n",
            " 'results': {'compute': {'Avg. CPU time (s) for 18 executions of Attack': 38.582432140722226,\n",
            "                         'Avg. CPU time (s) for 18 executions of Inference': 0.5737966902220756},\n",
            "             'metrics': {'adversarial_mean_categorical_accuracy': [0.8642857142857143],\n",
            "                         'benign_mean_categorical_accuracy': [0.9607142857142857],\n",
            "                         'perturbation_mean_linf': [0.010000020265579224]}},\n",
            " 'timestamp': 1693317673}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from charmory.engine import Engine\n",
        "from pprint import pprint\n",
        "\n",
        "evaluation = make_evaluation(epsilon=0.01)\n",
        "engine = Engine(evaluation)\n",
        "results = engine.run()\n",
        "pprint(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
