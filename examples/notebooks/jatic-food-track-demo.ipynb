{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before starting anything, we begin tracking the evaluation with MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.track import track_evaluation\n",
        "\n",
        "NAME=\"jatic-food-demo\"\n",
        "DESCRIPTION=\"Tracked food category classification from HuggingFace via JATIC-toolbox\"\n",
        "\n",
        "active_run = track_evaluation(NAME, description=DESCRIPTION)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by loading the model from HuggingFace using the JATIC-toolbox. We also\n",
        "use `track_params` to have all function arguments recorded with MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/msw/.virtualenvs/cdao/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import jatic_toolbox\n",
        "\n",
        "from charmory.track import track_params\n",
        "\n",
        "model = track_params(jatic_toolbox.load_model)(\n",
        "    provider=\"huggingface\",\n",
        "    model_name=\"Kaludi/food-category-classification-v2.0\",\n",
        "    task=\"image-classification\"\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model returns a `HuggingFaceProbs` object type, but ART expects the model output to be the `y` tensor. So we have to adapt the model to produce the correct output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.utils import adapt_jatic_image_classification_model_for_art\n",
        "\n",
        "adapt_jatic_image_classification_model_for_art(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then wrap it in an ART classifier to make it compatible with Armory/ART.\n",
        "Since we are instantiating a class, we use `track_init_params` to have the\n",
        "object initialization arguments logged with MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from art.estimators.classification import PyTorchClassifier\n",
        "import torch\n",
        "\n",
        "from charmory.track import track_init_params\n",
        "\n",
        "classifier = track_init_params(PyTorchClassifier)(\n",
        "    model,\n",
        "    loss=torch.nn.CrossEntropyLoss(),\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.003),\n",
        "    input_shape=(224, 224, 3),\n",
        "    channels_first=False,\n",
        "    nb_classes=12,\n",
        "    clip_values=(0.0, 1.0),\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we load the dataset from from HuggingFace using the JATIC-toolbox."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/msw/.virtualenvs/cdao/lib/python3.10/site-packages/datasets/load.py:2083: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbd246af7d4c4bb2869bac349d66caea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/1201 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fe1ff5ad8a240d6af209863f19adaf3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/303 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = track_params(jatic_toolbox.load_dataset)(\n",
        "    provider=\"huggingface\",\n",
        "    dataset_name=\"Kaludi/food-category-classification-v2.0\",\n",
        "    task=\"image-classification\",\n",
        "    split=\"validation\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this dataset contains bad images that will result in errors during evaluation, we will apply a filter to the underlying HuggingFace dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length prior to filtering: 300\n",
            "Dataset length after filtering: 280\n"
          ]
        }
      ],
      "source": [
        "from transformers.image_utils import infer_channel_dimension_format\n",
        "import numpy as np\n",
        "\n",
        "def filter(sample):\n",
        "    try:\n",
        "        infer_channel_dimension_format(np.asarray(sample[\"image\"]))\n",
        "        return True\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        return False\n",
        "\n",
        "print(f\"Dataset length prior to filtering: {len(dataset)}\")\n",
        "dataset._dataset = dataset._dataset.filter(filter)\n",
        "print(f\"Dataset length after filtering: {len(dataset)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then prepare a transform for the data using the preprocessor that comes with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.utils import create_jatic_image_classification_dataset_transform\n",
        "\n",
        "transform = create_jatic_image_classification_dataset_transform(model.preprocessor)\n",
        "dataset.set_transform(transform)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create an Armory data generator around the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.data import JaticVisionDatasetGenerator\n",
        "\n",
        "generator = JaticVisionDatasetGenerator(\n",
        "    dataset=dataset,\n",
        "    batch_size=16,\n",
        "    epochs=1,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly we will define the Armory evaluation, including the attack and scenario to be run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art.attacks.evasion\n",
        "from charmory.evaluation import (\n",
        "    Attack,\n",
        "    Dataset,\n",
        "    Evaluation,\n",
        "    Metric,\n",
        "    Model,\n",
        "    Scenario,\n",
        "    SysConfig,\n",
        ")\n",
        "import charmory.scenarios.image_classification\n",
        "\n",
        "def make_evaluation(epsilon: float) -> Evaluation:\n",
        "\n",
        "    eval_dataset = Dataset(\n",
        "        name=\"food-category-classification\",\n",
        "        test_dataset=generator,\n",
        "    )\n",
        "\n",
        "    eval_model = Model(\n",
        "        name=\"food-category-classification\",\n",
        "        model=classifier,\n",
        "    )\n",
        "\n",
        "    eval_attack = Attack(\n",
        "        name=\"PGD\",\n",
        "        attack=track_init_params(art.attacks.evasion.ProjectedGradientDescent)(\n",
        "            classifier,\n",
        "            batch_size=1,\n",
        "            eps=0.031,\n",
        "            eps_step=0.007,\n",
        "            max_iter=20,\n",
        "            num_random_init=1,\n",
        "            random_eps=False,\n",
        "            targeted=False,\n",
        "            verbose=False,\n",
        "        ),\n",
        "        use_label_for_untargeted=True,\n",
        "    )\n",
        "\n",
        "    eval_scenario = Scenario(\n",
        "        function=charmory.scenarios.image_classification.ImageClassificationTask,\n",
        "        kwargs={},\n",
        "    )\n",
        "\n",
        "    eval_metric = Metric(\n",
        "        profiler_type=\"basic\",\n",
        "        supported_metrics=[\"accuracy\"],\n",
        "        perturbation=[\"linf\"],\n",
        "        task=[\"categorical_accuracy\"],\n",
        "        means=True,\n",
        "        record_metric_per_sample=False,\n",
        "    )\n",
        "\n",
        "    eval_sysconfig = SysConfig(\n",
        "        gpus=[\"all\"],\n",
        "        use_gpu=True,\n",
        "    )\n",
        "\n",
        "    return Evaluation(\n",
        "        name=NAME,\n",
        "        description=DESCRIPTION,\n",
        "        author=\"Kaludi\",\n",
        "        dataset=eval_dataset,\n",
        "        model=eval_model,\n",
        "        attack=eval_attack,\n",
        "        scenario=eval_scenario,\n",
        "        metric=eval_metric,\n",
        "        sysconfig=eval_sysconfig,\n",
        "    )\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create an engine for the evaluation and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation:   0%|          | 0/18 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|██████████| 18/18 [06:38<00:00, 22.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-24 13:00:53 6m38s \u001b[34mMETRIC  \u001b[0m \u001b[36marmory.instrument.instrument\u001b[0m:\u001b[36m_write\u001b[0m:\u001b[36m742\u001b[0m benign_mean_categorical_accuracy on benign examples w.r.t. ground truth labels: 0.961\n",
            "2023-08-24 13:00:53 6m38s \u001b[34mMETRIC  \u001b[0m \u001b[36marmory.instrument.instrument\u001b[0m:\u001b[36m_write\u001b[0m:\u001b[36m742\u001b[0m adversarial_mean_categorical_accuracy on adversarial examples w.r.t. ground truth labels: 0.482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'armory_version': '23.7.2.post84+g5f24005e',\n",
              " 'evaluation': Evaluation(name='jatic-food-demo', description='Tracked food category classification from HuggingFace via JATIC-toolbox', model=Model(name='food-category-classification', model=art.estimators.classification.pytorch.PyTorchClassifier(model=ModelWrapper(\n",
              "   (_model): HuggingFaceImageClassifier(\n",
              "     (model): SwinForImageClassification(\n",
              "       (swin): SwinModel(\n",
              "         (embeddings): SwinEmbeddings(\n",
              "           (patch_embeddings): SwinPatchEmbeddings(\n",
              "             (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
              "           )\n",
              "           (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "           (dropout): Dropout(p=0.0, inplace=False)\n",
              "         )\n",
              "         (encoder): SwinEncoder(\n",
              "           (layers): ModuleList(\n",
              "             (0): SwinStage(\n",
              "               (blocks): ModuleList(\n",
              "                 (0-1): 2 x SwinLayer(\n",
              "                   (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "                   (attention): SwinAttention(\n",
              "                     (self): SwinSelfAttention(\n",
              "                       (query): Linear(in_features=128, out_features=128, bias=True)\n",
              "                       (key): Linear(in_features=128, out_features=128, bias=True)\n",
              "                       (value): Linear(in_features=128, out_features=128, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                     (output): SwinSelfOutput(\n",
              "                       (dense): Linear(in_features=128, out_features=128, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                   )\n",
              "                   (drop_path): SwinDropPath(p=0.1)\n",
              "                   (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "                   (intermediate): SwinIntermediate(\n",
              "                     (dense): Linear(in_features=128, out_features=512, bias=True)\n",
              "                     (intermediate_act_fn): GELUActivation()\n",
              "                   )\n",
              "                   (output): SwinOutput(\n",
              "                     (dense): Linear(in_features=512, out_features=128, bias=True)\n",
              "                     (dropout): Dropout(p=0.0, inplace=False)\n",
              "                   )\n",
              "                 )\n",
              "               )\n",
              "               (downsample): SwinPatchMerging(\n",
              "                 (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
              "                 (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "               )\n",
              "             )\n",
              "             (1): SwinStage(\n",
              "               (blocks): ModuleList(\n",
              "                 (0-1): 2 x SwinLayer(\n",
              "                   (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "                   (attention): SwinAttention(\n",
              "                     (self): SwinSelfAttention(\n",
              "                       (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "                       (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "                       (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                     (output): SwinSelfOutput(\n",
              "                       (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                   )\n",
              "                   (drop_path): SwinDropPath(p=0.1)\n",
              "                   (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "                   (intermediate): SwinIntermediate(\n",
              "                     (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
              "                     (intermediate_act_fn): GELUActivation()\n",
              "                   )\n",
              "                   (output): SwinOutput(\n",
              "                     (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
              "                     (dropout): Dropout(p=0.0, inplace=False)\n",
              "                   )\n",
              "                 )\n",
              "               )\n",
              "               (downsample): SwinPatchMerging(\n",
              "                 (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
              "                 (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "               )\n",
              "             )\n",
              "             (2): SwinStage(\n",
              "               (blocks): ModuleList(\n",
              "                 (0-17): 18 x SwinLayer(\n",
              "                   (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "                   (attention): SwinAttention(\n",
              "                     (self): SwinSelfAttention(\n",
              "                       (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "                       (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "                       (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                     (output): SwinSelfOutput(\n",
              "                       (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                   )\n",
              "                   (drop_path): SwinDropPath(p=0.1)\n",
              "                   (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "                   (intermediate): SwinIntermediate(\n",
              "                     (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "                     (intermediate_act_fn): GELUActivation()\n",
              "                   )\n",
              "                   (output): SwinOutput(\n",
              "                     (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "                     (dropout): Dropout(p=0.0, inplace=False)\n",
              "                   )\n",
              "                 )\n",
              "               )\n",
              "               (downsample): SwinPatchMerging(\n",
              "                 (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
              "                 (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "               )\n",
              "             )\n",
              "             (3): SwinStage(\n",
              "               (blocks): ModuleList(\n",
              "                 (0-1): 2 x SwinLayer(\n",
              "                   (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                   (attention): SwinAttention(\n",
              "                     (self): SwinSelfAttention(\n",
              "                       (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                       (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                       (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                     (output): SwinSelfOutput(\n",
              "                       (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "                       (dropout): Dropout(p=0.0, inplace=False)\n",
              "                     )\n",
              "                   )\n",
              "                   (drop_path): SwinDropPath(p=0.1)\n",
              "                   (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "                   (intermediate): SwinIntermediate(\n",
              "                     (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "                     (intermediate_act_fn): GELUActivation()\n",
              "                   )\n",
              "                   (output): SwinOutput(\n",
              "                     (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "                     (dropout): Dropout(p=0.0, inplace=False)\n",
              "                   )\n",
              "                 )\n",
              "               )\n",
              "             )\n",
              "           )\n",
              "         )\n",
              "         (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "         (pooler): AdaptiveAvgPool1d(output_size=1)\n",
              "       )\n",
              "       (classifier): Linear(in_features=1024, out_features=12, bias=True)\n",
              "     )\n",
              "   )\n",
              " ), loss=CrossEntropyLoss(), optimizer=Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     capturable: False\n",
              "     differentiable: False\n",
              "     eps: 1e-08\n",
              "     foreach: None\n",
              "     fused: None\n",
              "     lr: 0.003\n",
              "     maximize: False\n",
              "     weight_decay: 0\n",
              " ), input_shape=(224, 224, 3), nb_classes=12, channels_first=False, clip_values=array([0., 1.], dtype=float32), preprocessing_defences=None, postprocessing_defences=None, preprocessing=StandardisationMeanStdPyTorch(mean=0.0, std=1.0, apply_fit=True, apply_predict=True, device=cuda:0)), predict_kwargs={}), scenario=Scenario(function=<class 'charmory.scenarios.image_classification.ImageClassificationTask'>, kwargs={}, export_batches=False), dataset=Dataset(name='food-category-classification', test_dataset=<charmory.data.JaticVisionDatasetGenerator object at 0x7f7d4c3947f0>, train_dataset=None), author='Kaludi', attack=Attack(name='PGD', attack=<art.attacks.evasion.projected_gradient_descent.projected_gradient_descent.ProjectedGradientDescent object at 0x7f7ef8573700>, generate_kwargs={}, use_label_for_untargeted=True, label_targeter=None), metric=Metric(profiler_type='basic', supported_metrics=['accuracy'], perturbation=['linf'], task=['categorical_accuracy'], means=True, record_metric_per_sample=False), sysconfig=SysConfig(gpus=['all'], use_gpu=True, paths={'armory_home': PosixPath('/home/msw/.armory'), 'dataset_dir': PosixPath('/home/msw/.armory/datasets'), 'saved_model_dir': PosixPath('/home/msw/.armory/saved_models'), 'output_dir': PosixPath('/home/msw/.armory/outputs'), 'external_repo_dir': PosixPath('/home/msw/.armory/tmp/external')}, armory_home=PosixPath('/home/msw/.armory'))),\n",
              " 'results': {'metrics': {'benign_mean_categorical_accuracy': [0.9607142857142857],\n",
              "   'adversarial_mean_categorical_accuracy': [0.48214285714285715],\n",
              "   'perturbation_mean_linf': [0.03100001811981201]},\n",
              "  'compute': {'Avg. CPU time (s) for 18 executions of Inference': 0.17991455938873616,\n",
              "   'Avg. CPU time (s) for 18 executions of Attack': 21.477131005833346}},\n",
              " 'timestamp': 1692896055}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from charmory.engine import Engine\n",
        "\n",
        "evaluation = make_evaluation(epsilon=0.01)\n",
        "engine = Engine(evaluation)\n",
        "results = engine.run()\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly we stop the MLFlow run to finalize all records. When not in a notebook,\n",
        "this can be done automatically when using the `track_evaluation` as a context\n",
        "manager:\n",
        "\n",
        "```python\n",
        "with track_evaluation(NAME, description=DESCRIPTION):\n",
        "    ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
