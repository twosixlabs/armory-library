{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by loading the model from HuggingFace using the JATIC-toolbox. We also\n",
        "use `track_params` to have all function arguments recorded with MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/chris/transformers/src/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import jatic_toolbox\n",
        "\n",
        "from charmory.track import track_params\n",
        "\n",
        "model = track_params(jatic_toolbox.load_model)(\n",
        "    provider=\"huggingface\",\n",
        "    model_name=\"Kaludi/food-category-classification-v2.0\",\n",
        "    task=\"image-classification\"\n",
        ")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model returns a `HuggingFaceProbs` object type, but ART expects the model output to be the `y` tensor. So we have to wrap the model to produce the correct output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.model.image_classification import JaticImageClassificationModel\n",
        "\n",
        "wrapped_model = JaticImageClassificationModel(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then wrap it in an ART classifier to make it compatible with Armory/ART.\n",
        "Since we are instantiating a class, we use `track_init_params` to have the\n",
        "object initialization arguments logged with MLFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from art.estimators.classification import PyTorchClassifier\n",
        "import torch\n",
        "\n",
        "from charmory.track import track_init_params\n",
        "\n",
        "classifier = track_init_params(PyTorchClassifier)(\n",
        "    wrapped_model,\n",
        "    loss=torch.nn.CrossEntropyLoss(),\n",
        "    optimizer=torch.optim.Adam(model.parameters(), lr=0.003),\n",
        "    input_shape=(224, 224, 3),\n",
        "    channels_first=False,\n",
        "    nb_classes=12,\n",
        "    clip_values=(0.0, 1.0),\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we load the dataset from from HuggingFace using the JATIC-toolbox."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-09-08 16:05:44 12s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mdatasets.builder\u001b[0m:\u001b[36mdownload_and_prepare\u001b[0m:\u001b[36m835\u001b[0m Found cached dataset imagefolder (/home/chris/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f)\n",
            "2023-09-08 16:05:45 12s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mdatasets.arrow_dataset\u001b[0m:\u001b[36mmap\u001b[0m:\u001b[36m3076\u001b[0m Loading cached processed dataset at /home/chris/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-b82aa949443f821a.arrow\n"
          ]
        }
      ],
      "source": [
        "dataset = track_params(jatic_toolbox.load_dataset)(\n",
        "    provider=\"huggingface\",\n",
        "    dataset_name=\"Kaludi/food-category-classification-v2.0\",\n",
        "    task=\"image-classification\",\n",
        "    split=\"validation\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since this dataset contains bad images that will result in errors during evaluation, we will apply a filter to the underlying HuggingFace dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset length prior to filtering: 300\n",
            "2023-09-08 16:05:45 12s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36mdatasets.arrow_dataset\u001b[0m:\u001b[36mmap\u001b[0m:\u001b[36m3076\u001b[0m Loading cached processed dataset at /home/chris/.cache/huggingface/datasets/Kaludi___imagefolder/Kaludi--food-category-classification-v2.0-5568940526567eda/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f/cache-9d66eb47dd3bd531.arrow\n",
            "Dataset length after filtering: 280\n"
          ]
        }
      ],
      "source": [
        "from transformers.image_utils import infer_channel_dimension_format\n",
        "import numpy as np\n",
        "\n",
        "def filter(sample):\n",
        "    try:\n",
        "        infer_channel_dimension_format(np.asarray(sample[\"image\"]))\n",
        "        return True\n",
        "    except Exception as err:\n",
        "        print(err)\n",
        "        return False\n",
        "\n",
        "print(f\"Dataset length prior to filtering: {len(dataset)}\")\n",
        "dataset._dataset = dataset._dataset.filter(filter)\n",
        "print(f\"Dataset length after filtering: {len(dataset)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then prepare a transform for the data using the preprocessor that comes with the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.utils import create_jatic_dataset_transform\n",
        "\n",
        "transform = create_jatic_dataset_transform(model.preprocessor)\n",
        "dataset.set_transform(transform)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create an Armory data generator around the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from charmory.data import ArmoryDataLoader\n",
        "\n",
        "generator = ArmoryDataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=16,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lastly we will define the Armory evaluation, including the attack and scenario to be run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import art.attacks.evasion\n",
        "from armory.metrics.compute import BasicProfiler\n",
        "from charmory.evaluation import (\n",
        "    Attack,\n",
        "    Dataset,\n",
        "    Evaluation,\n",
        "    Metric,\n",
        "    Model,\n",
        "    SysConfig,\n",
        ")\n",
        "\n",
        "def make_evaluation(epsilon: float) -> Evaluation:\n",
        "\n",
        "    eval_dataset = Dataset(\n",
        "        name=\"food-category-classification\",\n",
        "        x_key=\"image\",\n",
        "        y_key=\"label\",\n",
        "        test_dataloader=generator,\n",
        "    )\n",
        "\n",
        "    eval_model = Model(\n",
        "        name=\"food-category-classification\",\n",
        "        model=classifier,\n",
        "    )\n",
        "\n",
        "    eval_attack = Attack(\n",
        "        name=\"PGD\",\n",
        "        attack=track_init_params(art.attacks.evasion.ProjectedGradientDescent)(\n",
        "            classifier,\n",
        "            batch_size=1,\n",
        "            eps=epsilon,\n",
        "            eps_step=0.007,\n",
        "            max_iter=20,\n",
        "            num_random_init=1,\n",
        "            random_eps=False,\n",
        "            targeted=False,\n",
        "            verbose=False,\n",
        "        ),\n",
        "        use_label_for_untargeted=True,\n",
        "    )\n",
        "\n",
        "    eval_metric = Metric(\n",
        "        profiler=BasicProfiler(),\n",
        "    )\n",
        "\n",
        "    eval_sysconfig = SysConfig(\n",
        "        gpus=[\"all\"],\n",
        "        use_gpu=True,\n",
        "    )\n",
        "\n",
        "    return  Evaluation(\n",
        "        name=\"jatic-food-demo\",\n",
        "        description=\"Tracked food category classification from HuggingFace via JATIC-toolbox\",\n",
        "        author=\"Kaludi\",\n",
        "        dataset=eval_dataset,\n",
        "        model=eval_model,\n",
        "        attack=eval_attack,\n",
        "        metric=eval_metric,\n",
        "        sysconfig=eval_sysconfig,\n",
        "    )\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now create an engine for the evaluation and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "You are using a CUDA device ('NVIDIA RTX A1000 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d07ebbdda08444aea7c56f9a230942d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:55 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m23s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m24s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m24s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m24s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m24s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n",
            "2023-09-08 16:16:56 11m24s \u001b[33m\u001b[1mWARNING \u001b[0m \u001b[36marmory.instrument.export\u001b[0m:\u001b[36mget_sample\u001b[0m:\u001b[36m87\u001b[0m Image out of expected range. Clipping to [0, 1].\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">      attack_accuracy      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8999999761581421     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">      benign_accuracy      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9750000238418579     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">       perturbation        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.010000020265579224    </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m     attack_accuracy     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8999999761581421    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m     benign_accuracy     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9750000238418579    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m      perturbation       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.010000020265579224   \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'compute': {'Avg. CPU time (s) for 5 executions of Inference': 1.3570806789997731, 'Avg. CPU time (s) for 5 executions of Attack': 129.93677240039978}, 'metrics': {'benign_accuracy': tensor(0.9750), 'attack_accuracy': tensor(0.9000), 'perturbation': tensor(0.0100)}}\n"
          ]
        }
      ],
      "source": [
        "from charmory.engine import EvaluationEngine\n",
        "from charmory.tasks.image_classification import ImageClassificationTask\n",
        "\n",
        "evaluation = make_evaluation(epsilon=0.01)\n",
        "\n",
        "task = ImageClassificationTask(evaluation, num_classes=12 , export_every_n_batches=5)\n",
        "engine = EvaluationEngine(task, limit_test_batches=5)\n",
        "results = engine.run()\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
