{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE = 'cpu'\n",
    "BATCH_SIZE = 1\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "#import armory.data.datasets\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\n",
    "    11: 0,  # Fixed-wing Aircraft\n",
    "    12: 1,  # Small Aircraft\n",
    "    13: 2,  # Cargo Plane\n",
    "    15: 3,  # Helicopter\n",
    "    17: 4,  # Passenger Vehicle\n",
    "    18: 5,  # Small Car\n",
    "    19: 6,  # Bus\n",
    "    20: 7,  # Pickup Truck\n",
    "    21: 8,  # Utility Truck\n",
    "    23: 9,  # Truck\n",
    "    24: 10,  # Cargo Truck\n",
    "    25: 11,  # Truck w/Box\n",
    "    26: 12,  # Truck Tractor\n",
    "    27: 13,  # Trailer\n",
    "    28: 14,  # Truck w/Flatbed\n",
    "    29: 15,  # Truck w/Liquid\n",
    "    32: 16,  # Crane Truck\n",
    "    33: 17,  # Railway Vehicle\n",
    "    34: 18,  # Passenger Car\n",
    "    35: 19,  # Cargo Car\n",
    "    36: 20,  # Flat Car\n",
    "    37: 21,  # Tank Car\n",
    "    38: 22,  # Locomotive\n",
    "    40: 23,  # Maritime Vessel\n",
    "    41: 24,  # Motorboat\n",
    "    42: 25,  # Sailboat\n",
    "    44: 26,  # Tugboat\n",
    "    45: 27,  # Barge\n",
    "    47: 28,  # Fishing Vessel\n",
    "    49: 29,  # Ferry\n",
    "    50: 30,  # Yacht\n",
    "    51: 31,  # Container Ship\n",
    "    52: 32,  # Oil Tanker\n",
    "    53: 33,  # Engineering Vehicle\n",
    "    54: 34,  # Tower Crane\n",
    "    55: 35,  # Container Crane\n",
    "    56: 36,  # Reach Stacker\n",
    "    57: 37,  # Straddle Carrier\n",
    "    59: 38,  # Mobile Crane\n",
    "    60: 39,  # Dump Truck\n",
    "    61: 40,  # Haul Truck\n",
    "    62: 41,  # Scraper/Tractor\n",
    "    63: 42,  # Front Loader/Bulldozer\n",
    "    64: 43,  # Excavator\n",
    "    65: 44,  # Cement Mixer\n",
    "    66: 45,  # Ground Grader\n",
    "    71: 46,  # Hut/Tent\n",
    "    72: 47,  # Shed\n",
    "    73: 48,  # Building\n",
    "    74: 49,  # Aircraft Hanger\n",
    "    75: 50,  # Unknown1\n",
    "    76: 51,  # Damaged Building\n",
    "    77: 52,  # Facility\n",
    "    79: 53,  # Construction Site\n",
    "    82: 54,  # Unknown2\n",
    "    83: 55,  # Vehicle Lot\n",
    "    84: 56,  # Helipad\n",
    "    86: 57,  # Storage Tank\n",
    "    89: 58,  # Shipping Container Lot\n",
    "    91: 59,  # Shipping Container\n",
    "    93: 60,  # Pylon\n",
    "    94: 61,  # Tower\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/chris/.cache/huggingface/datasets/Honaker___parquet/Honaker--xview_dataset-4b2c80dc283ac8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Loading cached split indices for dataset at /home/chris/.cache/huggingface/datasets/Honaker___parquet/Honaker--xview_dataset-4b2c80dc283ac8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-39dde171fdd76164.arrow and /home/chris/.cache/huggingface/datasets/Honaker___parquet/Honaker--xview_dataset-4b2c80dc283ac8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-e21aaff49de2d824.arrow\n"
     ]
    }
   ],
   "source": [
    "def load_huggingface_dataset():\n",
    "    train_data = datasets.load_dataset(\"Honaker/xview_dataset\", split=\"train\")\n",
    "    new_dataset = train_data.train_test_split(test_size=0.3, seed=1)\n",
    "    #new_dataset = train_data['train'].train_test_split(test_size=0.5, seed=1)\n",
    "    train_dataset, test_dataset = new_dataset[\"train\"], new_dataset[\"test\"]\n",
    "\n",
    "    #train_dataset, test_dataset = HuggingFaceObjectDetectionDataset(\n",
    "    #    train_dataset\n",
    "    #), HuggingFaceObjectDetectionDataset(test_dataset)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "train_dataset, test_dataset = load_huggingface_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        #A.Flip(0.5),\n",
    "        #A.RandomRotate90(0.5),\n",
    "        #A.MotionBlur(p=0.2),\n",
    "        #A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        #A.Blur(blur_limit=3, p=0.1),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc',\n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "# define the validation transforms\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc', \n",
    "        'label_fields': ['labels']\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "#from torchvision import tv_tensors\n",
    "import PIL\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "import cv2\n",
    "\n",
    "\n",
    "class xview(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset, width, height,  transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.image_dataset = dataset\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.image_dataset[idx]\n",
    "\n",
    "\n",
    "        image = np.asarray(img['image'])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[0]\n",
    "\n",
    "        boxes = img['objects']['bbox']\n",
    "        box_final = []\n",
    "        for box in boxes:\n",
    "            xmin_final = (box[0]/image_width)*self.width\n",
    "            xmax_final = (box[2]/image_width)*self.width\n",
    "            ymin_final = (box[1]/image_height)*self.height\n",
    "            yamx_final = (box[3]/image_height)*self.height\n",
    "            box_final.append([xmin_final, ymin_final, xmax_final, yamx_final])\n",
    "\n",
    "\n",
    "        boxes = torch.as_tensor(box_final, dtype=torch.float32)\n",
    "        area = img['objects']['area']\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        labels = img['objects']['category']\n",
    "        labels = list(\n",
    "                map(lambda x: LABEL_MAP[x], labels)\n",
    "            )  # map original classes to sequential classes\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"bboxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        #target[\"area\"] = area\n",
    "        #target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"img_size\"] =  (self.height, self.width),\n",
    "        target[\"img_scale\"] = torch.tensor([1.0]),\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = image_resized,\n",
    "                                     bboxes = target['bboxes'],\n",
    "                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "            target['bboxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return image_resized, target, image_id\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "\n",
    "def get_train_transforms(target_img_size=512):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            #A.HorizontalFlip(p=0.5),\n",
    "            A.Resize(height=target_img_size, width=target_img_size, p=1),\n",
    "            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(p=1),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"pascal_voc\", min_area=0, min_visibility=0, label_fields=[\"labels\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transforms(target_img_size=512):\n",
    "    return A.Compose(\n",
    "        [\n",
    "            #A.Resize(height=target_img_size, width=target_img_size, p=1),\n",
    "            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(p=1),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"pascal_voc\", min_area=0, min_visibility=0, label_fields=[\"labels\"]\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class EfficientDetDataModule(LightningDataModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                train_dataset_adaptor,\n",
    "                validation_dataset_adaptor,\n",
    "                train_transforms=get_train_transform(),\n",
    "                valid_transforms=get_valid_transform(),\n",
    "                num_workers=4,\n",
    "                height=100,\n",
    "                width=100,\n",
    "                batch_size=8):\n",
    "        \n",
    "        self.train_ds = train_dataset_adaptor\n",
    "        self.valid_ds = validation_dataset_adaptor\n",
    "        self.train_tfms = train_transforms\n",
    "        self.valid_tfms = valid_transforms\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        super().__init__()\n",
    "\n",
    "    def train_dataset(self) -> xview:\n",
    "        return xview(dataset=self.train_ds, width=self.width,\n",
    "                      height=self.height,  transforms=self.train_tfms   \n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        train_dataset = self.train_dataset()\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataset(self) -> xview:\n",
    "        return xview(dataset=self.valid_ds, width=self.width,\n",
    "                      height=self.height,  transforms=self.valid_tfms   \n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        valid_dataset = self.val_dataset()\n",
    "        valid_loader = torch.utils.data.DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "        return valid_loader\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        \n",
    "        images, targets, image_ids = tuple(zip(*batch))\n",
    "        images = torch.stack(images)\n",
    "        images = images.float()\n",
    "\n",
    "        boxes = [target[\"bboxes\"].float() for target in targets]\n",
    "        labels = [target[\"labels\"].float() for target in targets]\n",
    "        img_size = torch.tensor([target[\"img_size\"] for target in targets]).float()\n",
    "        img_scale = torch.tensor([target[\"img_scale\"] for target in targets]).float()\n",
    "\n",
    "        annotations = {\n",
    "            \"bbox\": boxes,\n",
    "            \"cls\": labels,\n",
    "            \"img_size\": img_size,\n",
    "            \"img_scale\": img_scale,\n",
    "        }\n",
    "        #return tuple(zip(*batch))\n",
    "\n",
    "        return images, annotations, targets, image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    num_classes = num_classes   # 63 classes\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "#from pytorch_lightning.core.decorators import auto_move_data\n",
    "\n",
    "class EfficientDetModel(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes=63,\n",
    "        img_size=512,\n",
    "        prediction_confidence_threshold=0.2,\n",
    "        learning_rate=0.0002,\n",
    "        wbf_iou_threshold=0.44,\n",
    "        inference_transforms=get_valid_transform, #get_valid_transforms(target_img_size=512),\n",
    "        model_architecture='tf_efficientnetv2_l',\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.model = create_model(num_classes)\n",
    "        self.prediction_confidence_threshold = prediction_confidence_threshold\n",
    "        self.lr = learning_rate\n",
    "        self.wbf_iou_threshold = wbf_iou_threshold\n",
    "        self.inference_tfms = inference_transforms\n",
    "\n",
    "\n",
    "    #@auto_move_data\n",
    "    def forward(self, images, targets):\n",
    "        return self.model(images, targets)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, annotations, _, image_ids = batch\n",
    "\n",
    "        losses = self.model(images, annotations)\n",
    "\n",
    "        '''      logging_losses = {\n",
    "            \"class_loss\": losses[\"class_loss\"].detach(),\n",
    "            \"box_loss\": losses[\"box_loss\"].detach(),\n",
    "        }'''\n",
    "\n",
    "        self.log(\"train_loss\", losses[\"loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
    "                 logger=True)\n",
    "        self.log(\n",
    "            \"train_class_loss\", losses[\"class_loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
    "            logger=True\n",
    "        )\n",
    "        self.log(\"train_box_loss\", losses[\"box_loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
    "                 logger=True)\n",
    "\n",
    "        return losses['loss']\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, annotations, targets, image_ids = batch\n",
    "        outputs = self.model(images, annotations)\n",
    "\n",
    "        detections = outputs[\"detections\"]\n",
    "\n",
    "        batch_predictions = {\n",
    "            \"predictions\": detections,\n",
    "            \"targets\": targets,\n",
    "            \"image_ids\": image_ids,\n",
    "        }\n",
    "\n",
    "        logging_losses = {\n",
    "            \"class_loss\": outputs[\"class_loss\"].detach(),\n",
    "            \"box_loss\": outputs[\"box_loss\"].detach(),\n",
    "        }\n",
    "\n",
    "        self.log(\"valid_loss\", outputs[\"loss\"], on_step=True, on_epoch=True, prog_bar=True,\n",
    "                 logger=True, sync_dist=True)\n",
    "        self.log(\n",
    "            \"valid_class_loss\", logging_losses[\"class_loss\"], on_step=True, on_epoch=True,\n",
    "            prog_bar=True, logger=True, sync_dist=True\n",
    "        )\n",
    "        self.log(\"valid_box_loss\", logging_losses[\"box_loss\"], on_step=True, on_epoch=True,\n",
    "                 prog_bar=True, logger=True, sync_dist=True)\n",
    "\n",
    "        return {'loss': outputs[\"loss\"], 'batch_predictions': batch_predictions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(max_epochs=5,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eff = EfficientDetModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataload = EfficientDetDataModule(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.EfficientDetDataModule at 0x7f3014406590>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | FasterRCNN | 41.6 M\n",
      "-------------------------------------\n",
      "41.4 M    Trainable params\n",
      "222 K     Non-trainable params\n",
      "41.6 M    Total params\n",
      "166.447   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cd59f867614655814eeee3db3a7db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model_eff,train_dataloaders\u001b[39m=\u001b[39;49mdataload\u001b[39m.\u001b[39;49mtrain_dataloader(), \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m                 val_dataloaders\u001b[39m=\u001b[39;49mdataload\u001b[39m.\u001b[39;49mval_dataloader())\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    533\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    534\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    982\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1021\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m   1020\u001b[0m     \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1021\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1023\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1050\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1047\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1049\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1052\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1054\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:376\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    375\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 376\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    378\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    380\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:293\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    295\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    296\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:393\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    392\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 393\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb Cell 20\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m     images, annotations, targets, image_ids \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(images, annotations)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     detections \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mdetections\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m     batch_predictions \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m: detections,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m\"\u001b[39m: targets,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimage_ids\u001b[39m\u001b[39m\"\u001b[39m: image_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training_lightning.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     }\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:83\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     77\u001b[0m     torch\u001b[39m.\u001b[39m_assert(\n\u001b[1;32m     78\u001b[0m         \u001b[39mlen\u001b[39m(val) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[1;32m     79\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpecting the last two dimensions of the Tensor to be H and W instead got \u001b[39m\u001b[39m{\u001b[39;00mimg\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     80\u001b[0m     )\n\u001b[1;32m     81\u001b[0m     original_image_sizes\u001b[39m.\u001b[39mappend((val[\u001b[39m0\u001b[39m], val[\u001b[39m1\u001b[39m]))\n\u001b[0;32m---> 83\u001b[0m images, targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(images, targets)\n\u001b[1;32m     85\u001b[0m \u001b[39m# Check for degenerate boxes\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39m# TODO: Move this to a function\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/models/detection/transform.py:119\u001b[0m, in \u001b[0;36mGeneralizedRCNNTransform.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets:\n\u001b[1;32m    118\u001b[0m     data: Dict[\u001b[39mstr\u001b[39m, Tensor] \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39;49mitems():\n\u001b[1;32m    120\u001b[0m         data[k] \u001b[39m=\u001b[39m v\n\u001b[1;32m    121\u001b[0m     targets_copy\u001b[39m.\u001b[39mappend(data)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "trainer.fit(model_eff,train_dataloaders=dataload.train_dataloader(), \n",
    "                val_dataloaders=dataload.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 211\n",
      "Number of validation samples: 212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset_1 = xview(train_dataset, 50, 50,  transforms=get_train_transform())\n",
    "test_dataset_1 = xview(test_dataset, 50, 50,  transforms=get_valid_transform())\n",
    "train_loader = create_train_loader(train_dataset_1)\n",
    "valid_loader = create_valid_loader(test_dataset_1)\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(test_dataset)}\\n\")\n",
    "# initialize the model and move to the computation device\n",
    "model = create_model(63)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# function for running training iterations\n",
    "def train(train_data_loader, model):\n",
    "    print('Training')\n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "    \n",
    "     # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        #train_loss_list.append(loss_value)\n",
    "        #train_loss_hist.send(loss_value)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_data_loader, model):\n",
    "    print('Validating')\n",
    "    global val_itr\n",
    "    global val_loss_list\n",
    "    \n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "        val_loss_hist.send(loss_value)\n",
    "        val_itr += 1\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 1\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c3995dcedd4e41bf4d4dc7caa4a99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650820df4210402897dc58dd45bdcf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa32117ac20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chris/.virtualenvs/cdao/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/chris/.virtualenvs/cdao/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa32117ac20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chris/.virtualenvs/cdao/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/chris/.virtualenvs/cdao/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.42 GiB (GPU 0; 4.00 GiB total capacity; 27.89 GiB already allocated; 0 bytes free; 31.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m train_loss \u001b[39m=\u001b[39m train(train_loader, model)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m val_loss \u001b[39m=\u001b[39m validate(valid_loader, model)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")   \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32m/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m loss_value \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrpn(images, features, targets)\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroi_heads(features, proposals, images\u001b[39m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:378\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtargets should not be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 378\u001b[0m labels, matched_gt_boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massign_targets_to_anchors(anchors, targets)\n\u001b[1;32m    379\u001b[0m regression_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_coder\u001b[39m.\u001b[39mencode(matched_gt_boxes, anchors)\n\u001b[1;32m    380\u001b[0m loss_objectness, loss_rpn_box_reg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(\n\u001b[1;32m    381\u001b[0m     objectness, pred_bbox_deltas, labels, regression_targets\n\u001b[1;32m    382\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:207\u001b[0m, in \u001b[0;36mRegionProposalNetwork.assign_targets_to_anchors\u001b[0;34m(self, anchors, targets)\u001b[0m\n\u001b[1;32m    205\u001b[0m     labels_per_image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((anchors_per_image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     match_quality_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbox_similarity(gt_boxes, anchors_per_image)\n\u001b[1;32m    208\u001b[0m     matched_idxs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproposal_matcher(match_quality_matrix)\n\u001b[1;32m    209\u001b[0m     \u001b[39m# get the targets corresponding GT for each proposal\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[39m# NB: need to clamp the indices because we can have a single\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[39m# GT in the image, and matched_idxs can be -2, which goes\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[39m# out of bounds\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/ops/boxes.py:271\u001b[0m, in \u001b[0;36mbox_iou\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[1;32m    270\u001b[0m     _log_api_usage_once(box_iou)\n\u001b[0;32m--> 271\u001b[0m inter, union \u001b[39m=\u001b[39m _box_inter_union(boxes1, boxes2)\n\u001b[1;32m    272\u001b[0m iou \u001b[39m=\u001b[39m inter \u001b[39m/\u001b[39m union\n\u001b[1;32m    273\u001b[0m \u001b[39mreturn\u001b[39;00m iou\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/ops/boxes.py:250\u001b[0m, in \u001b[0;36m_box_inter_union\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    247\u001b[0m wh \u001b[39m=\u001b[39m _upcast(rb \u001b[39m-\u001b[39m lt)\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# [N,M,2]\u001b[39;00m\n\u001b[1;32m    248\u001b[0m inter \u001b[39m=\u001b[39m wh[:, :, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m wh[:, :, \u001b[39m1\u001b[39m]  \u001b[39m# [N,M]\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m union \u001b[39m=\u001b[39m area1[:, \u001b[39mNone\u001b[39;49;00m] \u001b[39m+\u001b[39;49m area2 \u001b[39m-\u001b[39;49m inter\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m inter, union\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.42 GiB (GPU 0; 4.00 GiB total capacity; 27.89 GiB already allocated; 0 bytes free; 31.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import time\n",
    "NUM_EPOCHS= 1\n",
    "# get the model parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "# initialize the Averager class\n",
    "#train_loss_hist = Averager()\n",
    "#val_loss_hist = Averager()\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "# train and validation loss lists to store loss values of all...\n",
    "# ... iterations till ena and plot graphs for all iterations\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "# name to save the trained model with\n",
    "MODEL_NAME = 'model'\n",
    "\n",
    "# initialize SaveBestModel class\n",
    "#save_best_model = SaveBestModel()\n",
    "# start the training epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "    # reset the training and validation loss histories for the current epoch\n",
    "    #train_loss_hist.reset()\n",
    "    #val_loss_hist.reset()\n",
    "    # start timer and carry out training and validation\n",
    "    start = time.time()\n",
    "    train_loss = train(train_loader, model)\n",
    "    val_loss = validate(valid_loader, model)\n",
    "    #print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \n",
    "    #print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")   \n",
    "    end = time.time()\n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "    # save the best model till now if we have the least loss in the...\n",
    "    # ... current epoch\n",
    "    save_best_model(\n",
    "        val_loss_hist.value, epoch, model, optimizer\n",
    "    )\n",
    "    # save the current epoch model\n",
    "    save_model(epoch, model, optimizer)\n",
    "    # save loss plot\n",
    "    save_loss_plot(OUT_DIR, train_loss, val_loss)\n",
    "    \n",
    "    # sleep for 5 seconds after each epoch\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
