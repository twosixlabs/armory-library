{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE = 'cpu'\n",
    "BATCH_SIZE = 1\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "#import armory.data.datasets\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\n",
    "    11: 1,  # Fixed-wing Aircraft\n",
    "    12: 2,  # Small Aircraft\n",
    "    13: 3,  # Cargo Plane\n",
    "    15: 4,  # Helicopter\n",
    "    17: 5,  # Passenger Vehicle\n",
    "    18: 6,  # Small Car\n",
    "    19: 7,  # Bus\n",
    "    20: 8,  # Pickup Truck\n",
    "    21: 9,  # Utility Truck\n",
    "    23: 10,  # Truck\n",
    "    24: 11,  # Cargo Truck\n",
    "    25: 12,  # Truck w/Box\n",
    "    26: 13,  # Truck Tractor\n",
    "    27: 14,  # Trailer\n",
    "    28: 15,  # Truck w/Flatbed\n",
    "    29: 16,  # Truck w/Liquid\n",
    "    32: 17,  # Crane Truck\n",
    "    33: 18,  # Railway Vehicle\n",
    "    34: 19,  # Passenger Car\n",
    "    35: 20,  # Cargo Car\n",
    "    36: 21,  # Flat Car\n",
    "    37: 22,  # Tank Car\n",
    "    38: 23,  # Locomotive\n",
    "    40: 24,  # Maritime Vessel\n",
    "    41: 25,  # Motorboat\n",
    "    42: 26,  # Sailboat\n",
    "    44: 27,  # Tugboat\n",
    "    45: 28,  # Barge\n",
    "    47: 29,  # Fishing Vessel\n",
    "    49: 30,  # Ferry\n",
    "    50: 31,  # Yacht\n",
    "    51: 32,  # Container Ship\n",
    "    52: 33,  # Oil Tanker\n",
    "    53: 34,  # Engineering Vehicle\n",
    "    54: 35,  # Tower Crane\n",
    "    55: 36,  # Container Crane\n",
    "    56: 37,  # Reach Stacker\n",
    "    57: 38,  # Straddle Carrier\n",
    "    59: 39,  # Mobile Crane\n",
    "    60: 40,  # Dump Truck\n",
    "    61: 41,  # Haul Truck\n",
    "    62: 42,  # Scraper/Tractor\n",
    "    63: 43,  # Front Loader/Bulldozer\n",
    "    64: 44,  # Excavator\n",
    "    65: 45,  # Cement Mixer\n",
    "    66: 46,  # Ground Grader\n",
    "    71: 47,  # Hut/Tent\n",
    "    72: 48,  # Shed\n",
    "    73: 49,  # Building\n",
    "    74: 50,  # Aircraft Hanger\n",
    "    75: 51,  # Unknown1\n",
    "    76: 52,  # Damaged Building\n",
    "    77: 53,  # Facility\n",
    "    79: 54,  # Construction Site\n",
    "    82: 55,  # Unknown2\n",
    "    83: 56,  # Vehicle Lot\n",
    "    84: 57,  # Helipad\n",
    "    86: 58,  # Storage Tank\n",
    "    89: 59,  # Shipping Container Lot\n",
    "    91: 60,  # Shipping Container\n",
    "    93: 61,  # Pylon\n",
    "    94: 62,  # Tower\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/chris/.cache/huggingface/datasets/Honaker___parquet/Honaker--xview_dataset-4b2c80dc283ac8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Loading cached split indices for dataset at /home/chris/.cache/huggingface/datasets/Honaker___parquet/Honaker--xview_dataset-4b2c80dc283ac8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-8763adf942e98fc0.arrow and /home/chris/.cache/huggingface/datasets/Honaker___parquet/Honaker--xview_dataset-4b2c80dc283ac8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7/cache-52f107d6deaa0fbe.arrow\n"
     ]
    }
   ],
   "source": [
    "def load_huggingface_dataset():\n",
    "    train_data = datasets.load_dataset(\"Honaker/xview_dataset\", split=\"train\")\n",
    "    train_data = train_data.train_test_split(test_size=0.5, seed=1)\n",
    "    new_dataset = train_data['train'].train_test_split(test_size=0.5, seed=1)\n",
    "    train_dataset, test_dataset = new_dataset[\"train\"], new_dataset[\"test\"]\n",
    "\n",
    "    #train_dataset, test_dataset = HuggingFaceObjectDetectionDataset(\n",
    "    #    train_dataset\n",
    "    #), HuggingFaceObjectDetectionDataset(test_dataset)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "train_dataset, test_dataset = load_huggingface_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    To handle the data loading as different images may have different number \n",
    "    of objects and to handle varying size tensors as well.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        #A.Flip(0.5),\n",
    "        #A.RandomRotate90(0.5),\n",
    "        #A.MotionBlur(p=0.2),\n",
    "        #A.MedianBlur(blur_limit=3, p=0.1),\n",
    "        #A.Blur(blur_limit=3, p=0.1),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc',\n",
    "        'label_fields': ['labels']\n",
    "    })\n",
    "# define the validation transforms\n",
    "def get_valid_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ], bbox_params={\n",
    "        'format': 'pascal_voc', \n",
    "        'label_fields': ['labels']\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/chris/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "#from torchvision import tv_tensors\n",
    "import PIL\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "import cv2\n",
    "\n",
    "\n",
    "class xview(torch.utils.data.Dataset):\n",
    "    def __init__(self,dataset, width, height,  transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.image_dataset = dataset\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.image_dataset[idx]\n",
    "\n",
    "\n",
    "        image = np.asarray(img['image'])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image_resized = cv2.resize(image, (self.width, self.height))\n",
    "        image_resized /= 255.0\n",
    "        image_width = image.shape[1]\n",
    "        image_height = image.shape[0]\n",
    "\n",
    "        boxes = img['objects']['bbox']\n",
    "        box_final = []\n",
    "        for box in boxes:\n",
    "            xmin_final = (box[0]/image_width)*self.width\n",
    "            xmax_final = (box[2]/image_width)*self.width\n",
    "            ymin_final = (box[1]/image_height)*self.height\n",
    "            yamx_final = (box[3]/image_height)*self.height\n",
    "            box_final.append([xmin_final, ymin_final, xmax_final, yamx_final])\n",
    "\n",
    "\n",
    "        boxes = torch.as_tensor(box_final, dtype=torch.float32)\n",
    "        area = img['objects']['area']\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        labels = img['objects']['category']\n",
    "        labels = list(\n",
    "                map(lambda x: LABEL_MAP[x], labels)\n",
    "            )  # map original classes to sequential classes\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        image_id = torch.tensor([idx])\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image = image_resized,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            image_resized = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "            \n",
    "        return image_resized, target\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "def create_train_loader(train_dataset, num_workers=2):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return train_loader\n",
    "def create_valid_loader(valid_dataset, num_workers=2):\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    num_classes = num_classes   # 63 classes\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 211\n",
      "Number of validation samples: 212\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset_1 = xview(train_dataset, 50, 50,  transforms=get_train_transform())\n",
    "test_dataset_1 = xview(test_dataset, 50, 50,  transforms=get_valid_transform())\n",
    "train_loader = create_train_loader(train_dataset_1)\n",
    "valid_loader = create_valid_loader(test_dataset_1)\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(test_dataset)}\\n\")\n",
    "# initialize the model and move to the computation device\n",
    "model = create_model(63)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# function for running training iterations\n",
    "def train(train_data_loader, model):\n",
    "    print('Training')\n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "    \n",
    "     # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        #train_loss_list.append(loss_value)\n",
    "        #train_loss_hist.send(loss_value)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_data_loader, model):\n",
    "    print('Validating')\n",
    "    global val_itr\n",
    "    global val_loss_list\n",
    "    \n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "        val_loss_hist.send(loss_value)\n",
    "        val_itr += 1\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 1\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c3995dcedd4e41bf4d4dc7caa4a99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650820df4210402897dc58dd45bdcf35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa32117ac20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chris/.virtualenvs/cdao/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/chris/.virtualenvs/cdao/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa32117ac20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chris/.virtualenvs/cdao/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/chris/.virtualenvs/cdao/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1461, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.42 GiB (GPU 0; 4.00 GiB total capacity; 27.89 GiB already allocated; 0 bytes free; 31.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m train_loss \u001b[39m=\u001b[39m train(train_loader, model)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m val_loss \u001b[39m=\u001b[39m validate(valid_loader, model)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")   \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32m/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/chris/armory/examples/src/charmory_examples/xview_training.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m loss_value \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrpn(images, features, targets)\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroi_heads(features, proposals, images\u001b[39m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:378\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtargets should not be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 378\u001b[0m labels, matched_gt_boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massign_targets_to_anchors(anchors, targets)\n\u001b[1;32m    379\u001b[0m regression_targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_coder\u001b[39m.\u001b[39mencode(matched_gt_boxes, anchors)\n\u001b[1;32m    380\u001b[0m loss_objectness, loss_rpn_box_reg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(\n\u001b[1;32m    381\u001b[0m     objectness, pred_bbox_deltas, labels, regression_targets\n\u001b[1;32m    382\u001b[0m )\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:207\u001b[0m, in \u001b[0;36mRegionProposalNetwork.assign_targets_to_anchors\u001b[0;34m(self, anchors, targets)\u001b[0m\n\u001b[1;32m    205\u001b[0m     labels_per_image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((anchors_per_image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     match_quality_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbox_similarity(gt_boxes, anchors_per_image)\n\u001b[1;32m    208\u001b[0m     matched_idxs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproposal_matcher(match_quality_matrix)\n\u001b[1;32m    209\u001b[0m     \u001b[39m# get the targets corresponding GT for each proposal\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     \u001b[39m# NB: need to clamp the indices because we can have a single\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[39m# GT in the image, and matched_idxs can be -2, which goes\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[39m# out of bounds\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/ops/boxes.py:271\u001b[0m, in \u001b[0;36mbox_iou\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[1;32m    270\u001b[0m     _log_api_usage_once(box_iou)\n\u001b[0;32m--> 271\u001b[0m inter, union \u001b[39m=\u001b[39m _box_inter_union(boxes1, boxes2)\n\u001b[1;32m    272\u001b[0m iou \u001b[39m=\u001b[39m inter \u001b[39m/\u001b[39m union\n\u001b[1;32m    273\u001b[0m \u001b[39mreturn\u001b[39;00m iou\n",
      "File \u001b[0;32m~/.virtualenvs/cdao/lib/python3.10/site-packages/torchvision/ops/boxes.py:250\u001b[0m, in \u001b[0;36m_box_inter_union\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    247\u001b[0m wh \u001b[39m=\u001b[39m _upcast(rb \u001b[39m-\u001b[39m lt)\u001b[39m.\u001b[39mclamp(\u001b[39mmin\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# [N,M,2]\u001b[39;00m\n\u001b[1;32m    248\u001b[0m inter \u001b[39m=\u001b[39m wh[:, :, \u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m wh[:, :, \u001b[39m1\u001b[39m]  \u001b[39m# [N,M]\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m union \u001b[39m=\u001b[39m area1[:, \u001b[39mNone\u001b[39;49;00m] \u001b[39m+\u001b[39;49m area2 \u001b[39m-\u001b[39;49m inter\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m inter, union\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.42 GiB (GPU 0; 4.00 GiB total capacity; 27.89 GiB already allocated; 0 bytes free; 31.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import time\n",
    "NUM_EPOCHS= 1\n",
    "# get the model parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# define the optimizer\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "# initialize the Averager class\n",
    "#train_loss_hist = Averager()\n",
    "#val_loss_hist = Averager()\n",
    "train_itr = 1\n",
    "val_itr = 1\n",
    "# train and validation loss lists to store loss values of all...\n",
    "# ... iterations till ena and plot graphs for all iterations\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "# name to save the trained model with\n",
    "MODEL_NAME = 'model'\n",
    "\n",
    "# initialize SaveBestModel class\n",
    "#save_best_model = SaveBestModel()\n",
    "# start the training epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "    # reset the training and validation loss histories for the current epoch\n",
    "    #train_loss_hist.reset()\n",
    "    #val_loss_hist.reset()\n",
    "    # start timer and carry out training and validation\n",
    "    start = time.time()\n",
    "    train_loss = train(train_loader, model)\n",
    "    val_loss = validate(valid_loader, model)\n",
    "    #print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \n",
    "    #print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")   \n",
    "    end = time.time()\n",
    "    print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "    # save the best model till now if we have the least loss in the...\n",
    "    # ... current epoch\n",
    "    save_best_model(\n",
    "        val_loss_hist.value, epoch, model, optimizer\n",
    "    )\n",
    "    # save the current epoch model\n",
    "    save_model(epoch, model, optimizer)\n",
    "    # save loss plot\n",
    "    save_loss_plot(OUT_DIR, train_loss, val_loss)\n",
    "    \n",
    "    # sleep for 5 seconds after each epoch\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
