{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch vision example\n",
    "\n",
    "Source: https://www.learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/\n",
    "with minor modifications. Errors are likely mine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch\n",
    "\n",
    "# Let us look at the Deep learning architectures implemented in the torch vision library.\n",
    "dir(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is one entry called **AlexNet** and one called **alexnet**. The capitalised name refers to the Python class (AlexNet) whereas alexnet is a convenience function that returns the model instantiated from the AlexNet class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.alexnet import AlexNet_Weights\n",
    "\n",
    "alexnet = models.alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "print(alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import json\n",
    "\n",
    "def imagenet_class(idx: int, cache=None):\n",
    "    if cache is None:\n",
    "        cache = json.load(open(Path('./imagenet_classes.json')))[\"imagenet_classes\"]\n",
    "        assert len(cache) == 1000 and cache[11] == 'goldfinch'\n",
    "\n",
    "    return cache[idx]\n",
    "\n",
    "transform = transforms.Compose([        # Defining a variable transforms\n",
    "   transforms.Resize(256),                # Resize the image to 256×256 pixels\n",
    "   transforms.CenterCrop(224),            # Crop the image to 224×224 pixels about the center\n",
    "   transforms.ToTensor(),                 # Convert the image to PyTorch Tensor data type\n",
    "   transforms.Normalize(                  # Normalize the image\n",
    "   mean=[0.485, 0.456, 0.406],            # Mean and std of image as also used when training the network\n",
    "   std=[0.229, 0.224, 0.225]\n",
    " )])\n",
    "\n",
    "def evaluate_image(img):\n",
    "    img_t = transform(img)                # Apply the transformations on the image\n",
    "    batch_t = torch.unsqueeze(img_t, 0)   # Add a batch dimension to the image\n",
    "    alexnet.eval()                        # Set the network to evaluation mode\n",
    "    out = alexnet(batch_t)                # Forward propagate the image\n",
    "\n",
    "    _, indices = torch.sort(out, descending=True)\n",
    "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
    "    return [(imagenet_class(idx), percentage[idx].item()) for idx in indices[0][:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(Path('./dog.jpg'))\n",
    "best = evaluate_image(img)\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# running testing alexnet with stl10 torchvision dataset\n",
    "\n",
    "As it happens, the INaturalist dataset was good for our purposes but way too big.\n",
    "As [the STL-10 page notes](https://cs.stanford.edu/~acoates/stl10/) these\n",
    "images were drawn from ImageNet so presumably are readily recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charmory.evaluation import SysConfig\n",
    "from torchvision.datasets import STL10\n",
    "from pprint import pprint\n",
    "\n",
    "root = SysConfig().dataset_cache / 'stl10'\n",
    "stl = STL10(root=root, split='test', download=True)\n",
    "\n",
    "stl_classes = 'airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck'.split(', ')\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    img, label = stl[i]\n",
    "    target = stl_classes[label]\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(f\"{label=}, {target=}\")\n",
    "    best = evaluate_image(img)\n",
    "    pprint(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that didn't work. There are only 10 classes in STL-10 and the model outputs\n",
    "1000 classes. As shown in the first output from the cell above, the first STL item\n",
    "is a horse, but imagenet has no horse class. So we get the most likely as \"oxcart\"\n",
    "which isn't a bad guess, but it is not a horse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using Imagenet-1k dataset\n",
    "\n",
    "The Imagenet-1k dataset in torchvision requires tarfiles downloaded from image-net.org,\n",
    "but I've applied to get access and it takes \"up to 5 days\". So I've downloaded parquet\n",
    "files for the same dataset and have stashed them on our \n",
    "s3://armory-library-data/datasets/huggingface/imagenet/ so at least I don't have to\n",
    "hunt them down again.\n",
    "\n",
    "This means that I'm going to use a huggingface dataloader for now.\n",
    "\n",
    "Or perhaps not, let's try to keep to the same framework, so I'll use the torchvision\n",
    "VisionDataset class and see if I can get it to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using Pyarrow to create a torchvision.VisionDataset\n",
    "\n",
    "So what I've got is the HuggingFace parquet files, but I'd like them to look like a\n",
    "torchvision dataset. Thus I get make a pyarrow table from the parquet and wrap it in the\n",
    "`__init__` and `__getitem__` protocol of a `torchvision.VisionDataset`.\n",
    "\n",
    "This has all been encapsulated in the `imagenet_tst.ImageNetTST` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from imagenet_tst import get_local_imagenettst\n",
    "\n",
    "imtst = get_local_imagenettst(\"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image, label \u001b[38;5;241m=\u001b[39m \u001b[43mimtst\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimtst\u001b[38;5;241m.\u001b[39mlabel(label)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n",
      "File \u001b[0;32m~/charmory/examples/src/armory/examples/image_classification/torchvision/imagenet_tst.py:62\u001b[0m, in \u001b[0;36mImageNetTST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     60\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[index]\n\u001b[1;32m     61\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m---> 62\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m target \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "image, label = imtst[0]\n",
    "print(f\"{image=}, {label=} {imtst.label(label)=}\")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(90, 100):\n",
    "    img, label = imtst[i]\n",
    "    print(f\"{img=}, {label=}\")\n",
    "    target = imtst.label(label)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(f\"{label=}, {target=}\")\n",
    "    best = evaluate_image(img)\n",
    "    pprint(best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
